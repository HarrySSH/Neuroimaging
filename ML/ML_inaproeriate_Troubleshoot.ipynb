{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jingjinglab/cw/miniconda3/envs/Harry_env/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import lazypredict\n",
    "import mat4py\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import os \n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## \n",
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.96822965,  0.78817663,  0.72479129,  0.82032864,  0.92730691,\n",
       "        0.50795244,  0.82515398,  0.94652056,  1.        ,  0.98032987,\n",
       "        0.99550904,  0.97737923,  1.        ,  1.        ,  0.97699912,\n",
       "        0.98952854,  0.7394642 ,  0.59969775,  0.92784213,  0.94694797,\n",
       "        0.91149344,  0.80121812,  0.85247174,  0.76296672,  0.97651273,\n",
       "        0.63809483,  0.94366512,  0.91851984,  0.7514474 ,  0.76656555,\n",
       "        0.64465631,  0.80314625,  0.96465159,  0.75072608,  1.        ,\n",
       "        0.74731254,  0.6661933 ,  0.89200643,  0.52239879,  0.95840181,\n",
       "        0.53861451,  0.82771857,  0.95891329,  0.88437528,  1.        ,\n",
       "        0.97737065,  0.67072406,  0.66955341,  0.95206264,  0.92038178,\n",
       "        0.33013482, -0.15548849, -0.19372855,  0.06888124,  0.16074136,\n",
       "       -0.13670387,  0.02200773, -0.01813104,  0.24927392, -0.01967013,\n",
       "        0.2481965 ,  0.31118594,  0.10799357,  0.47760121,  0.01859732,\n",
       "        0.45091403, -0.08825437, -0.35921554,  0.04346685, -0.05305203,\n",
       "       -0.06587722,  0.13049406,  0.18291833, -0.18909593,  0.05613095,\n",
       "        0.18106995, -0.07818527, -0.10286294,  0.03808443,  0.08266438,\n",
       "       -0.10348706,  0.01173892, -0.00823957,  0.12450408, -0.00862689,\n",
       "        0.12452845,  0.1664421 ,  0.04962676,  0.28195814,  0.00834569,\n",
       "        0.26411354, -0.04895928, -0.20381979,  0.02083519, -0.02367145,\n",
       "       -0.03030254,  0.07719633,  0.10488085, -0.09614863,  0.02570718])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X_train)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "Connectomes = scipy.io.loadmat('../Cells2Connectomes/Connectomes.mat')\n",
    "Connectome_direct = Connectomes['C_dir']\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Region volumes, in a 424 vector, to get connectivity density, divide\n",
    "% each row in connectomes by each entry in the vector to get density. Units\n",
    "% are in 200 micron per vertex voxels.\n",
    "\n",
    "'''\n",
    "\n",
    "CellType_volumn = mat4py.loadmat('../Cells2Connectomes/Regional_Volumes.mat')\n",
    "CellType_volumn = CellType_volumn['region_vols']\n",
    "Celltype_volumn =np.array([np.array(xi) for xi in CellType_volumn])\n",
    "Celltype_volumn.shape\n",
    "\n",
    "# Nomarlize by the entry\n",
    "\n",
    "Connectome_direct_density = Connectome_direct/Celltype_volumn\n",
    "\n",
    "Cell_type = mat4py.loadmat('../Cells2Connectomes/CellType_Maps.mat')\n",
    "Cell_type = Cell_type['cellmaps']\n",
    "Celltype_mtx =np.array([np.array(xi) for xi in Cell_type])\n",
    "Celltype_mtx.shape\n",
    "\n",
    "# Important : normalizing via the columns\n",
    "\n",
    "Celltype_mtx_norm = (Celltype_mtx.max(axis=0)-Celltype_mtx) / (Celltype_mtx.max(axis=0) - Celltype_mtx.min(axis=0) )\n",
    "\n",
    "\n",
    "# region names\n",
    "Region_maps = mat4py.loadmat('../Cells2Connectomes/Region_Names.mat')\n",
    "Region_maps = Region_maps['region_names']\n",
    "Regionmaps_df = pd.DataFrame(Region_maps,columns = ['Anno1','Anno2','Anno3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anno1</th>\n",
       "      <th>Anno2</th>\n",
       "      <th>Anno3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Visual Cortex</td>\n",
       "      <td>Visual Cortex</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Gray Matter</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Anno1          Anno2      Anno3\n",
       "172  Visual Cortex  Visual Cortex  Brainstem\n",
       "173      Brainstem      Brainstem  Brainstem\n",
       "174      Brainstem      Brainstem  Brainstem\n",
       "175      Brainstem      Brainstem  Brainstem\n",
       "176      Brainstem      Brainstem  Brainstem\n",
       "177      Brainstem      Brainstem  Brainstem\n",
       "178      Brainstem      Brainstem  Brainstem\n",
       "179      Brainstem      Brainstem  Brainstem\n",
       "180      Brainstem      Brainstem  Brainstem\n",
       "181      Brainstem      Brainstem  Brainstem\n",
       "182      Brainstem      Brainstem  Brainstem\n",
       "183      Brainstem      Brainstem  Brainstem\n",
       "184      Brainstem      Brainstem  Brainstem\n",
       "185      Brainstem      Brainstem  Brainstem\n",
       "186      Brainstem      Brainstem  Brainstem\n",
       "187      Brainstem      Brainstem  Brainstem\n",
       "193    Gray Matter      Brainstem  Brainstem\n",
       "194      Brainstem      Brainstem  Brainstem\n",
       "195      Brainstem      Brainstem  Brainstem\n",
       "196      Brainstem      Brainstem  Brainstem"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Regionmaps_df[Regionmaps_df['Anno3'] =='Brainstem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(Regionmaps_df['Anno1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KF = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "KF.get_n_splits(list(range(0,424)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0021167075314507625"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(Celltype_mtx_norm[Celltype_mtx_norm!=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log10(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "\n",
    "# build a df to store the information\n",
    "\n",
    "# instead use the subset, this time I am trying the whole, which might be not suitable for ML, but I just wanted a positive control\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "res_df =[]\n",
    "\n",
    "\n",
    "Target = []\n",
    "Dataset = []\n",
    "for i in range(Celltype_mtx_norm.shape[0]):\n",
    "    for j in range(Celltype_mtx_norm.shape[0]):\n",
    "        #print(i)\n",
    "        #print(j)\n",
    "        if i == j:\n",
    "            pass       \n",
    "        else:\n",
    "\n",
    "            #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "            #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "            Dataset.append(np.concatenate((Celltype_mtx_norm[i,:],Celltype_mtx_norm[j,:],\n",
    "                                          Celltype_mtx_norm[i,:] - Celltype_mtx_norm[j,:], \n",
    "                                         np.log10((Celltype_mtx_norm[i,:]+ 0.0001 )/ (Celltype_mtx_norm[j,:]+0.0001)))))\n",
    "            Target.append(Connectome_direct_density[i,j])\n",
    "Dataset = np.stack(Dataset)\n",
    "Target =np.array([np.array(xi) for xi in Target])   \n",
    "    \n",
    "#X_train, X_test, y_train, y_test = train_test_split(Dataset, Target,test_size=.2,random_state =123)  \n",
    "\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle= True)\n",
    "list_r2 = []\n",
    "for train_index, test_index in kf.split(Dataset):\n",
    "    \n",
    "    \n",
    "    sub1 = train_index#), int(len(train_index))\n",
    "    sub2 = test_index#), int(len(test_index)/50))\n",
    "    X_train, X_test = Dataset[sub1], Dataset[sub2]\n",
    "    y_train, y_test = Target[sub1], Target[sub2]\n",
    "    \n",
    "    \n",
    "    RF_model = RandomForestRegressor(n_estimators=100,max_features = 50, warm_start = True)\n",
    "    RF_model.fit(X_train, y_train)\n",
    "    print('Traing fit')\n",
    "    Predict = RF_model.predict(X_train)\n",
    "    print(r2_score(y_train, Predict))\n",
    "    print('Testing fit')\n",
    "    Predict = RF_model.predict(X_test)\n",
    "    print(r2_score(y_test, Predict))\n",
    "    list_r2.append(r2_score(y_test, Predict))\n",
    "    \n",
    "    \n",
    "# split into 50 %\n",
    "#sub_1 = sample(list(range(X_train.shape[0])),int(X_train.shape[0]/10) )\n",
    "#sub_2 = sample(list(range(X_test.shape[0])),int(X_test.shape[0]/10) )\n",
    "# subset 50 percents\n",
    "#Dataset_Training_sub = X_train[sub_1,:]\n",
    "    \n",
    "#Target_training_sub = y_train[sub_1]\n",
    "    \n",
    "#Dataset_Testing_sub = X_test[sub_2,:]\n",
    "    \n",
    "#Target_Testing_sub = y_test[sub_2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.21992529285145568,\n",
       " 0.17196768142895924,\n",
       " 0.0892550271176844,\n",
       " 0.1736190164958975,\n",
       " 0.3478016754367037,\n",
       " 0.17150288480338083,\n",
       " 0.2503509627352718,\n",
       " 0.1662671732913782,\n",
       " 0.27098765056574725,\n",
       " 0.2559602726139455]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179352, 75)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adjusted R-Squared</th>\n",
       "      <th>R-Squared</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Adjusted R-Squared, R-Squared, RMSE, Time Taken]\n",
       "Index: []"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/42 [00:54<37:33, 54.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'AdaBoostRegressor', 'R-Squared': -6.27437131841446, 'Adjusted R-Squared': -6.284525382231341, 'RMSE': 9.787981565238232, 'Time taken': 54.95334315299988}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▍         | 2/42 [02:11<45:05, 67.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'BaggingRegressor', 'R-Squared': 0.13014442948374194, 'Adjusted R-Squared': 0.12893022572813584, 'RMSE': 3.384690670821487, 'Time taken': 76.49333715438843}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 3/42 [02:24<27:42, 42.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'BayesianRidge', 'R-Squared': 0.006551402320528088, 'Adjusted R-Squared': 0.005164678984850513, 'RMSE': 3.6171632548953054, 'Time taken': 12.88908314704895}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|▉         | 4/42 [02:34<18:55, 29.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'DecisionTreeRegressor', 'R-Squared': -0.38964014215530596, 'Adjusted R-Squared': -0.3915798966809274, 'RMSE': 4.278057183352493, 'Time taken': 10.30411148071289}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 5/42 [02:35<11:55, 19.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'DummyRegressor', 'R-Squared': -2.2339774563562287e-06, 'Adjusted R-Squared': -0.0013981053258336207, 'RMSE': 3.629074593502688, 'Time taken': 0.6887359619140625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 6/42 [02:36<07:59, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'ElasticNet', 'R-Squared': -2.2339774563562287e-06, 'Adjusted R-Squared': -0.0013981053258336207, 'RMSE': 3.629074593502688, 'Time taken': 1.6298069953918457}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 7/42 [03:18<13:04, 22.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'ElasticNetCV', 'R-Squared': 0.005796424697616653, 'Adjusted R-Squared': 0.0044086475126609015, 'RMSE': 3.618537437053481, 'Time taken': 41.13783526420593}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▉        | 8/42 [03:21<09:11, 16.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'ExtraTreeRegressor', 'R-Squared': -0.6901080674689013, 'Adjusted R-Squared': -0.6924672356256139, 'RMSE': 4.717942539604872, 'Time taken': 2.9797446727752686}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██▏       | 9/42 [08:07<55:23, 100.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'ExtraTreesRegressor', 'R-Squared': 0.10747014220054285, 'Adjusted R-Squared': 0.10622428812767937, 'RMSE': 3.4285207844571817, 'Time taken': 286.4595503807068}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 10/42 [08:08<37:13, 69.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GammaRegressor model failed to execute\n",
      "Some value(s) of y are out of the valid range for family GammaDistribution\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "\n",
    "# build a df to store the information\n",
    "\n",
    "# instead use the subset, this time I am trying the whole, which might be not suitable for ML, but I just wanted a positive control\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "res_df =[]\n",
    "\n",
    "\n",
    "Target = []\n",
    "Dataset = []\n",
    "for i in range(Celltype_mtx_norm.shape[0]):\n",
    "    for j in range(Celltype_mtx_norm.shape[0]):\n",
    "        #print(i)\n",
    "        #print(j)\n",
    "        if i == j:\n",
    "            pass       \n",
    "        else:\n",
    "\n",
    "            #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "            #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "            Dataset.append(np.concatenate((Celltype_mtx_norm[i,:],Celltype_mtx_norm[j,:])))\n",
    "            Target.append(Connectome_direct_density[i,j])\n",
    "Dataset = np.stack(Dataset)\n",
    "Target =np.array([np.array(xi) for xi in Target])   \n",
    "    \n",
    "#X_train, X_test, y_train, y_test = train_test_split(Dataset, Target,test_size=.2,random_state =123)  \n",
    "\n",
    "#\n",
    "kf = KFold(n_splits=5, shuffle= True)\n",
    "\n",
    "for train_index, test_index in kf.split(Dataset):\n",
    "    sub1 = train_index#), int(len(train_index)/50))\n",
    "    sub2 = test_index#), int(len(test_index)/50))\n",
    "    X_train, X_test = Dataset[sub1], Dataset[sub2]\n",
    "    y_train, y_test = Target[sub1], Target[sub2]\n",
    "  \n",
    "    \n",
    "    \n",
    "    reg = LazyRegressor(verbose=1, predictions=True,ignore_warnings=False, custom_metric=None)\n",
    "    models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
    "    print(models)\n",
    "    models.to_csv('res_inproper_all.csv', sep = '\\t')\n",
    "    break\n",
    "# split into 50 %\n",
    "#sub_1 = sample(list(range(X_train.shape[0])),int(X_train.shape[0]/10) )\n",
    "#sub_2 = sample(list(range(X_test.shape[0])),int(X_test.shape[0]/10) )\n",
    "# subset 50 percents\n",
    "#Dataset_Training_sub = X_train[sub_1,:]\n",
    "    \n",
    "#Target_training_sub = y_train[sub_1]\n",
    "    \n",
    "#Dataset_Testing_sub = X_test[sub_2,:]\n",
    "    \n",
    "#Target_Testing_sub = y_test[sub_2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3587,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RandomForestRegressor', 0.004659784996502658, 3.4779960071010225]\n",
      "['RandomForestRegressor', 0.006998202035632062, 3.4902083996326403]\n",
      "['RandomForestRegressor', 0.008233164737332377, 4.030816268865591]\n",
      "['RandomForestRegressor', 0.0065886139845724445, 3.258057572325783]\n",
      "['RandomForestRegressor', 0.007675450180245735, 3.4102158244164]\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "\n",
    "# build a df to store the information\n",
    "\n",
    "# instead use the subset, this time I am trying the whole, which might be not suitable for ML, but I just wanted a positive control\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "res_df =[]\n",
    "\n",
    "\n",
    "Target = []\n",
    "Dataset = []\n",
    "for i in range(Celltype_mtx_norm.shape[0]):\n",
    "    for j in range(Celltype_mtx_norm.shape[0]):\n",
    "        #print(i)\n",
    "        #print(j)\n",
    "        if i == j:\n",
    "            pass       \n",
    "        else:\n",
    "\n",
    "            #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "            #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "            Dataset.append(np.concatenate((Celltype_mtx_norm[i,:],Celltype_mtx_norm[j,:])))\n",
    "            Target.append(Connectome_direct_density[i,j])\n",
    "Dataset = np.stack(Dataset)\n",
    "Target =np.array([np.array(xi) for xi in Target])   \n",
    "    \n",
    "#X_train, X_test, y_train, y_test = train_test_split(Dataset, Target,test_size=.2,random_state =123)  \n",
    "\n",
    "#\n",
    "kf = KFold(n_splits=5, shuffle= True)\n",
    "\n",
    "for train_index, test_index in kf.split(Dataset):\n",
    "    X_train, X_test = Dataset[train_index], Dataset[test_index]\n",
    "    y_train, y_test = Target[train_index], Target[test_index]\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "    Predict = lr.predict(X_test)\n",
    "    _res = []\n",
    "    _res.append('lr')\n",
    "\n",
    "    _res.append(r2_score(y_test,Predict))\n",
    "    _res.append(mean_squared_error(y_test,Predict,squared=False))\n",
    "    print(_res)\n",
    "    \n",
    "    \n",
    "#reg = LazyRegressor(verbose=1, predictions=True,ignore_warnings=False, custom_metric=None)\n",
    "# split into 50 %\n",
    "#sub_1 = sample(list(range(X_train.shape[0])),int(X_train.shape[0]/10) )\n",
    "#sub_2 = sample(list(range(X_test.shape[0])),int(X_test.shape[0]/10) )\n",
    "# subset 50 percents\n",
    "#Dataset_Training_sub = X_train[sub_1,:]\n",
    "    \n",
    "#Target_training_sub = y_train[sub_1]\n",
    "    \n",
    "#Dataset_Testing_sub = X_test[sub_2,:]\n",
    "    \n",
    "#Target_Testing_sub = y_test[sub_2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____\n"
     ]
    }
   ],
   "source": [
    "# SVR\n",
    "\n",
    "# build a df to store the information\n",
    "\n",
    "# instead use the subset, this time I am trying the whole, which might be not suitable for ML, but I just wanted a positive control\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "res_df =[]\n",
    "\n",
    "\n",
    "Target = []\n",
    "Dataset = []\n",
    "for i in range(Celltype_mtx_norm.shape[0]):\n",
    "    for j in range(Celltype_mtx_norm.shape[0]):\n",
    "        #print(i)\n",
    "        #print(j)\n",
    "        if i == j:\n",
    "            pass       \n",
    "        else:\n",
    "\n",
    "            #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "            #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "            Dataset.append(np.concatenate((Celltype_mtx_norm[i,:],Celltype_mtx_norm[j,:])))\n",
    "            Target.append(Connectome_direct_density[i,j])\n",
    "Dataset = np.stack(Dataset)\n",
    "Target =np.array([np.array(xi) for xi in Target])   \n",
    "    \n",
    "#X_train, X_test, y_train, y_test = train_test_split(Dataset, Target,test_size=.2,random_state =123)  \n",
    "\n",
    "#\n",
    "kf = KFold(n_splits=5, shuffle= True)\n",
    "\n",
    "for train_index, test_index in kf.split(Dataset):\n",
    "    print('____')\n",
    "    X_train, X_test = Dataset[train_index], Dataset[test_index]\n",
    "    y_train, y_test = Target[train_index], Target[test_index]\n",
    "    svr = SVR()\n",
    "    svr.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "    Predict = svr.predict(X_test)\n",
    "    _res = []\n",
    "    _res.append('RandomForestRegressor')\n",
    "\n",
    "    _res.append(r2_score(y_test,Predict))\n",
    "    _res.append(mean_squared_error(y_test,Predict,squared=False))\n",
    "    print(_res)\n",
    "    \n",
    "    \n",
    "#reg = LazyRegressor(verbose=1, predictions=True,ignore_warnings=False, custom_metric=None)\n",
    "# split into 50 %\n",
    "#sub_1 = sample(list(range(X_train.shape[0])),int(X_train.shape[0]/10) )\n",
    "#sub_2 = sample(list(range(X_test.shape[0])),int(X_test.shape[0]/10) )\n",
    "# subset 50 percents\n",
    "#Dataset_Training_sub = X_train[sub_1,:]\n",
    "    \n",
    "#Target_training_sub = y_train[sub_1]\n",
    "    \n",
    "#Dataset_Testing_sub = X_test[sub_2,:]\n",
    "    \n",
    "#Target_Testing_sub = y_test[sub_2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96822965, 0.78817663, 0.72479129, 0.82032864, 0.92730691,\n",
       "       0.50795244, 0.82515398, 0.94652056, 1.        , 0.98032987,\n",
       "       0.99550904, 0.97737923, 1.        , 1.        , 0.97699912,\n",
       "       0.98952854, 0.7394642 , 0.59969775, 0.92784213, 0.94694797,\n",
       "       0.91149344, 0.80121812, 0.85247174, 0.76296672, 0.97651273,\n",
       "       0.49770912, 0.92353547, 0.98336786, 0.76338216, 0.93051494,\n",
       "       0.422534  , 0.72074844, 0.91873064, 0.66188623, 0.96662907,\n",
       "       0.92766184, 0.75139376, 0.97489687, 0.93668178, 0.99383866,\n",
       "       0.63769894, 0.80606318, 0.91386461, 0.69750952, 0.99709589,\n",
       "       0.91181692, 0.53702232, 0.72445541, 0.96431617, 0.95015779])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143481, 50)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35871, 50)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RandomForestRegressor', 0.25931770503665563, 3.115926466907411]\n"
     ]
    }
   ],
   "source": [
    "RF_Regmodel = RandomForestRegressor()\n",
    "RF_Regmodel.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "Predict = RF_Regmodel.predict(X_test)\n",
    "_res = []\n",
    "_res.append('RandomForestRegressor')\n",
    "    \n",
    "_res.append(r2_score(y_test,Predict))\n",
    "_res.append(mean_squared_error(y_test,Predict,squared=False))\n",
    "print(_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8865365991304476"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Predict = RF_Regmodel.predict(X_train[:])\n",
    "r2_score(y_train[:],Predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8967,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Target_Testing_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.04290075, 0.05129423, 0.00204868, 0.        ,\n",
       "       0.19620738, 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Target_training_sub[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10409774, 3.59019734, 0.24403804, 0.05332101, 0.16795146,\n",
       "       0.3863247 , 0.18396161, 3.47359509, 0.53028246, 1.40108984])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Predict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.60533880e-04, 0.00000000e+00, 1.52469704e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.11590916e-02, 1.58809255e+00])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Target_Testing_sub[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform the validation, iteration:1\n",
      "Build Training set\n",
      "Build Testing set\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "         \n",
    "    reg = LazyRegressor(verbose=1, predictions=True,ignore_warnings=False, custom_metric=None)\n",
    "    # split into 50 %\n",
    "    sub_1 = sample(list(range(Dataset_Training.shape[0])),int(Dataset_Training.shape[0]/40) )\n",
    "    sub_2 = sample(list(range(Dataset_Testing.shape[0])),int(Dataset_Testing.shape[0]/10) )\n",
    "    # subset 50 percents\n",
    "    Dataset_Training_sub = Dataset_Training[sub_1,:]\n",
    "    \n",
    "    Target_training_sub = Target_training[sub_1]\n",
    "    \n",
    "    Dataset_Testing_sub = Dataset_Testing[sub_2,:]\n",
    "    \n",
    "    Target_Testing_sub = Target_Testing[sub_2]\n",
    "    \n",
    "    #models, predictions = reg.fit(Dataset_Training_sub, Dataset_Testing_sub, Target_training_sub, Target_Testing_sub)\n",
    "    #print(models)\n",
    "    \n",
    "    # random forest reg\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    RF_Regmodel = RandomForestRegressor()\n",
    "    RF_Regmodel.fit(Dataset_Training_sub,Target_training_sub)\n",
    "\n",
    "\n",
    "    Predict = RF_Regmodel.predict(Dataset_Testing_sub)\n",
    "    _res = []\n",
    "    _res.append('RandomForestRegressor')\n",
    "    \n",
    "    _res.append(r2_score(Target_Testing_sub,Predict))\n",
    "    _res.append(mean_squared_error(Target_Testing_sub,Predict,squared=False))\n",
    "    res_df.append(_res)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform the validation, iteration:1\n",
      "Build Training set\n",
      "Build Testing set\n"
     ]
    }
   ],
   "source": [
    "# support vector\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "k = 0\n",
    "for train_index, test_index in KF.split(list(range(0,424))):\n",
    "    k = k +1\n",
    "    print('Perform the validation, iteration:'+str(k))\n",
    "    Targets_Train_set = Connectome_direct_density[:,train_index]\n",
    "    Targets_Train_set  = Targets_Train_set[train_index,:]\n",
    "    Targets_Test_set  = Connectome_direct_density[:,test_index]\n",
    "    Targets_Test_set   = Targets_Test_set[test_index,:]\n",
    "    \n",
    "    \n",
    "    Dataset_Train_set = Celltype_mtx_norm[train_index,:]\n",
    "    Dataset_Test_set  = Celltype_mtx_norm[test_index,:]\n",
    "    print('Build Training set')\n",
    "    Target_training = []\n",
    "    Dataset_Training = []\n",
    "    for i in range(Dataset_Train_set.shape[0]):\n",
    "        for j in range(Dataset_Train_set.shape[0]):\n",
    "            #print(i)\n",
    "            #print(j)\n",
    "            if i == j:\n",
    "                pass       \n",
    "            else:\n",
    "\n",
    "                #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "                #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "                Dataset_Training.append(np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:])))\n",
    "                Target_training.append(Targets_Train_set[i,j])\n",
    "    Dataset_Training = np.stack(Dataset_Training)\n",
    "    Target_training =np.array([np.array(xi) for xi in Target_training])   \n",
    "    \n",
    "    print('Build Testing set')\n",
    "    Target_Testing = []\n",
    "    Dataset_Testing = []\n",
    "    for i in range(Dataset_Test_set.shape[0]):\n",
    "        for j in range(Dataset_Test_set.shape[0]):\n",
    "            #print(i)\n",
    "            #print(j)\n",
    "            if i == j:\n",
    "                pass       \n",
    "            else:\n",
    "                #print(Dataset_Test_set[i,j])\n",
    "\n",
    "                #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "                #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "                Dataset_Testing.append(np.concatenate((Dataset_Test_set[i,:],Dataset_Test_set[j,:])))\n",
    "                Target_Testing.append(Targets_Test_set[i,j])\n",
    "    Dataset_Testing = np.stack(Dataset_Testing)\n",
    "    Target_Testing =np.array([np.array(xi) for xi in Target_Testing])      \n",
    "         \n",
    "    reg = LazyRegressor(verbose=1, predictions=True,ignore_warnings=False, custom_metric=None)\n",
    "    # split into 50 %\n",
    "    sub_1 = sample(list(range(Dataset_Training.shape[0])),int(Dataset_Training.shape[0]/40) )\n",
    "    sub_2 = sample(list(range(Dataset_Testing.shape[0])),int(Dataset_Testing.shape[0]/10) )\n",
    "    # subset 50 percents\n",
    "    Dataset_Training_sub = Dataset_Training[sub_1,:]\n",
    "    \n",
    "    Target_training_sub = Target_training[sub_1]\n",
    "    \n",
    "    Dataset_Testing_sub = Dataset_Testing[sub_2,:]\n",
    "    \n",
    "    Target_Testing_sub = Target_Testing[sub_2]\n",
    "    \n",
    "    #models, predictions = reg.fit(Dataset_Training_sub, Dataset_Testing_sub, Target_training_sub, Target_Testing_sub)\n",
    "    #print(models)\n",
    "    \n",
    "    # support vector regressor\n",
    "    \n",
    "    \n",
    "    svr_rbf = SVR()\n",
    "    \n",
    "    svr_rbf.fit(Dataset_Training_sub,Target_training_sub)\n",
    "\n",
    "\n",
    "    Predict = svr_rbf.predict(Dataset_Testing_sub)\n",
    "    _res = []\n",
    "    _res.append('SVR')\n",
    "    \n",
    "    _res.append(r2_score(Target_Testing_sub,Predict))\n",
    "    _res.append(mean_squared_error(Target_Testing_sub,Predict,squared=False))\n",
    "    res_df.append(_res)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform the validation, iteration:1\n",
      "Build Training set\n",
      "Build Testing set\n"
     ]
    }
   ],
   "source": [
    "# linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "k = 0\n",
    "for train_index, test_index in KF.split(list(range(0,424))):\n",
    "    k = k +1\n",
    "    print('Perform the validation, iteration:'+str(k))\n",
    "    Targets_Train_set = Connectome_direct_density[:,train_index]\n",
    "    Targets_Train_set  = Targets_Train_set[train_index,:]\n",
    "    Targets_Test_set  = Connectome_direct_density[:,test_index]\n",
    "    Targets_Test_set   = Targets_Test_set[test_index,:]\n",
    "    \n",
    "    \n",
    "    Dataset_Train_set = Celltype_mtx_norm[train_index,:]\n",
    "    Dataset_Test_set  = Celltype_mtx_norm[test_index,:]\n",
    "    print('Build Training set')\n",
    "    Target_training = []\n",
    "    Dataset_Training = []\n",
    "    for i in range(Dataset_Train_set.shape[0]):\n",
    "        for j in range(Dataset_Train_set.shape[0]):\n",
    "            #print(i)\n",
    "            #print(j)\n",
    "            if i == j:\n",
    "                pass       \n",
    "            else:\n",
    "\n",
    "                #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "                #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "                Dataset_Training.append(np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:])))\n",
    "                Target_training.append(Targets_Train_set[i,j])\n",
    "    Dataset_Training = np.stack(Dataset_Training)\n",
    "    Target_training =np.array([np.array(xi) for xi in Target_training])   \n",
    "    \n",
    "    print('Build Testing set')\n",
    "    Target_Testing = []\n",
    "    Dataset_Testing = []\n",
    "    for i in range(Dataset_Test_set.shape[0]):\n",
    "        for j in range(Dataset_Test_set.shape[0]):\n",
    "            #print(i)\n",
    "            #print(j)\n",
    "            if i == j:\n",
    "                pass       \n",
    "            else:\n",
    "                #print(Dataset_Test_set[i,j])\n",
    "\n",
    "                #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "                #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "                Dataset_Testing.append(np.concatenate((Dataset_Test_set[i,:],Dataset_Test_set[j,:])))\n",
    "                Target_Testing.append(Targets_Test_set[i,j])\n",
    "    Dataset_Testing = np.stack(Dataset_Testing)\n",
    "    Target_Testing =np.array([np.array(xi) for xi in Target_Testing])      \n",
    "         \n",
    "    reg = LazyRegressor(verbose=1, predictions=True,ignore_warnings=False, custom_metric=None)\n",
    "    # split into 50 %\n",
    "    sub_1 = sample(list(range(Dataset_Training.shape[0])),int(Dataset_Training.shape[0]/40) )\n",
    "    sub_2 = sample(list(range(Dataset_Testing.shape[0])),int(Dataset_Testing.shape[0]/10) )\n",
    "    # subset 50 percents\n",
    "    Dataset_Training_sub = Dataset_Training[sub_1,:]\n",
    "    \n",
    "    Target_training_sub = Target_training[sub_1]\n",
    "    \n",
    "    Dataset_Testing_sub = Dataset_Testing[sub_2,:]\n",
    "    \n",
    "    Target_Testing_sub = Target_Testing[sub_2]\n",
    "    \n",
    "    #models, predictions = reg.fit(Dataset_Training_sub, Dataset_Testing_sub, Target_training_sub, Target_Testing_sub)\n",
    "    #print(models)\n",
    "    \n",
    "    # support vector regressor\n",
    "    \n",
    "    \n",
    "    Lr = LinearRegression()\n",
    "    \n",
    "    Lr.fit(Dataset_Training_sub,Target_training_sub)\n",
    "\n",
    "\n",
    "    Predict = Lr.predict(Dataset_Testing_sub)\n",
    "    _res = []\n",
    "    _res.append('LinearRegression')\n",
    "    \n",
    "    _res.append(r2_score(Target_Testing_sub,Predict))\n",
    "    _res.append(mean_squared_error(Target_Testing_sub,Predict,squared=False))\n",
    "    res_df.append(_res)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "k = 0\n",
    "for train_index, test_index in KF.split(list(range(0,424))):\n",
    "    k = k +1\n",
    "    print('Perform the validation, iteration:'+str(k))\n",
    "    Targets_Train_set = Connectome_direct_density[:,train_index]\n",
    "    Targets_Train_set  = Targets_Train_set[train_index,:]\n",
    "    Targets_Test_set  = Connectome_direct_density[:,test_index]\n",
    "    Targets_Test_set   = Targets_Test_set[test_index,:]\n",
    "    \n",
    "    \n",
    "    Dataset_Train_set = Celltype_mtx_norm[train_index,:]\n",
    "    Dataset_Test_set  = Celltype_mtx_norm[test_index,:]\n",
    "    print('Build Training set')\n",
    "    Target_training = []\n",
    "    Dataset_Training = []\n",
    "    for i in range(Dataset_Train_set.shape[0]):\n",
    "        for j in range(Dataset_Train_set.shape[0]):\n",
    "            #print(i)\n",
    "            #print(j)\n",
    "            if i == j:\n",
    "                pass       \n",
    "            else:\n",
    "\n",
    "                #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "                #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "                Dataset_Training.append(np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:])))\n",
    "                Target_training.append(Targets_Train_set[i,j])\n",
    "    Dataset_Training = np.stack(Dataset_Training)\n",
    "    Target_training =np.array([np.array(xi) for xi in Target_training])   \n",
    "    \n",
    "    print('Build Testing set')\n",
    "    Target_Testing = []\n",
    "    Dataset_Testing = []\n",
    "    for i in range(Dataset_Test_set.shape[0]):\n",
    "        for j in range(Dataset_Test_set.shape[0]):\n",
    "            #print(i)\n",
    "            #print(j)\n",
    "            if i == j:\n",
    "                pass       \n",
    "            else:\n",
    "                #print(Dataset_Test_set[i,j])\n",
    "\n",
    "                #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "                #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "                Dataset_Testing.append(np.concatenate((Dataset_Test_set[i,:],Dataset_Test_set[j,:])))\n",
    "                Target_Testing.append(Targets_Test_set[i,j])\n",
    "    Dataset_Testing = np.stack(Dataset_Testing)\n",
    "    Target_Testing =np.array([np.array(xi) for xi in Target_Testing])      \n",
    "         \n",
    "    reg = LazyRegressor(verbose=1, predictions=True,ignore_warnings=False, custom_metric=None)\n",
    "    # split into 50 %\n",
    "    sub_1 = sample(list(range(Dataset_Training.shape[0])),int(Dataset_Training.shape[0]/40) )\n",
    "    sub_2 = sample(list(range(Dataset_Testing.shape[0])),int(Dataset_Testing.shape[0]/10) )\n",
    "    # subset 50 percents\n",
    "    Dataset_Training_sub = Dataset_Training[sub_1,:]\n",
    "    \n",
    "    Target_training_sub = Target_training[sub_1]\n",
    "    \n",
    "    Dataset_Testing_sub = Dataset_Testing[sub_2,:]\n",
    "    \n",
    "    Target_Testing_sub = Target_Testing[sub_2]\n",
    "    \n",
    "    #models, predictions = reg.fit(Dataset_Training_sub, Dataset_Testing_sub, Target_training_sub, Target_Testing_sub)\n",
    "    #print(models)\n",
    "    \n",
    "    # support vector regressor\n",
    "    \n",
    "    \n",
    "    Lr = LinearRegression()\n",
    "    \n",
    "    Lr.fit(Dataset_Training_sub,Target_training_sub)\n",
    "\n",
    "\n",
    "    Predict = Lr.predict(Dataset_Testing_sub)\n",
    "    _res = []\n",
    "    _res.append('LinearRegression')\n",
    "    \n",
    "    _res.append(r2_score(Target_Testing_sub,Predict))\n",
    "    _res.append(mean_squared_error(Target_Testing_sub,Predict,squared=False))\n",
    "    res_df.append(_res)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Methods</th>\n",
       "      <th>R_square</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>-2.17</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVR</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>1.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Methods  R_square  RMSE\n",
       "0  RandomForestRegressor     -2.17  2.75\n",
       "1                    SVR     -0.01  2.75\n",
       "2       LinearRegression     -0.09  1.73"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Result = pd.DataFrame(res_df,columns = ['Methods','R_square','RMSE'])\n",
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_Testing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\n",
    "models, predictions = reg.fit(Dataset_Training, Dataset_Testing, Target_training, Target_Testing)\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
