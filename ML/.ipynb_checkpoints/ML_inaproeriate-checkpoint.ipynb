{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jingjinglab/cw/miniconda3/envs/Harry_env/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import lazypredict\n",
    "import mat4py\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import os \n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## \n",
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "Connectomes = scipy.io.loadmat('../Cells2Connectomes/Connectomes.mat')\n",
    "Connectome_direct = Connectomes['C_dir']\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Region volumes, in a 424 vector, to get connectivity density, divide\n",
    "% each row in connectomes by each entry in the vector to get density. Units\n",
    "% are in 200 micron per vertex voxels.\n",
    "\n",
    "'''\n",
    "\n",
    "CellType_volumn = mat4py.loadmat('../Cells2Connectomes/Regional_Volumes.mat')\n",
    "CellType_volumn = CellType_volumn['region_vols']\n",
    "Celltype_volumn =np.array([np.array(xi) for xi in CellType_volumn])\n",
    "Celltype_volumn.shape\n",
    "\n",
    "# Nomarlize by the entry\n",
    "\n",
    "Connectome_direct_density = Connectome_direct/Celltype_volumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cell_type = mat4py.loadmat('../Cells2Connectomes/CellType_Maps.mat')\n",
    "Cell_type = Cell_type['cellmaps']\n",
    "Celltype_mtx =np.array([np.array(xi) for xi in Cell_type])\n",
    "Celltype_mtx.shape\n",
    "\n",
    "# Important : normalizing via the columns\n",
    "\n",
    "Celltype_mtx_norm = (Celltype_mtx.max(axis=0)-Celltype_mtx) / (Celltype_mtx.max(axis=0) - Celltype_mtx.min(axis=0) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Region_maps = mat4py.loadmat('../Cells2Connectomes/Region_Names.mat')\n",
    "Region_maps = Region_maps['region_names']\n",
    "Regionmaps_df = pd.DataFrame(Region_maps,columns = ['Anno1','Anno2','Anno3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KF = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "KF.get_n_splits(list(range(0,424)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform the validation, iteration:1\n",
      "Build Training set\n",
      "Build Testing set\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "\n",
    "# build a df to store the information\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "res_df =[]\n",
    "\n",
    "k = 0\n",
    "for train_index, test_index in KF.split(list(range(0,424))):\n",
    "    k = k +1\n",
    "    print('Perform the validation, iteration:'+str(k))\n",
    "    Targets_Train_set = Connectome_direct_density[:,train_index]\n",
    "    Targets_Train_set  = Targets_Train_set[train_index,:]\n",
    "    Targets_Test_set  = Connectome_direct_density[:,test_index]\n",
    "    Targets_Test_set   = Targets_Test_set[test_index,:]\n",
    "    \n",
    "    \n",
    "    Dataset_Train_set = Celltype_mtx_norm[train_index,:]\n",
    "    Dataset_Test_set  = Celltype_mtx_norm[test_index,:]\n",
    "    print('Build Training set')\n",
    "    Target_training = []\n",
    "    Dataset_Training = []\n",
    "    for i in range(Dataset_Train_set.shape[0]):\n",
    "        for j in range(Dataset_Train_set.shape[0]):\n",
    "            #print(i)\n",
    "            #print(j)\n",
    "            if i == j:\n",
    "                pass       \n",
    "            else:\n",
    "\n",
    "                #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "                #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "                Dataset_Training.append(np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:])))\n",
    "                Target_training.append(Targets_Train_set[i,j])\n",
    "    Dataset_Training = np.stack(Dataset_Training)\n",
    "    Target_training =np.array([np.array(xi) for xi in Target_training])   \n",
    "    \n",
    "    print('Build Testing set')\n",
    "    Target_Testing = []\n",
    "    Dataset_Testing = []\n",
    "    for i in range(Dataset_Test_set.shape[0]):\n",
    "        for j in range(Dataset_Test_set.shape[0]):\n",
    "            #print(i)\n",
    "            #print(j)\n",
    "            if i == j:\n",
    "                pass       \n",
    "            else:\n",
    "                #print(Dataset_Test_set[i,j])\n",
    "\n",
    "                #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "                #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "                Dataset_Testing.append(np.concatenate((Dataset_Test_set[i,:],Dataset_Test_set[j,:])))\n",
    "                Target_Testing.append(Targets_Test_set[i,j])\n",
    "    Dataset_Testing = np.stack(Dataset_Testing)\n",
    "    Target_Testing =np.array([np.array(xi) for xi in Target_Testing])      \n",
    "         \n",
    "    reg = LazyRegressor(verbose=1, predictions=True,ignore_warnings=False, custom_metric=None)\n",
    "    # split into 50 %\n",
    "    sub_1 = sample(list(range(Dataset_Training.shape[0])),int(Dataset_Training.shape[0]/40) )\n",
    "    sub_2 = sample(list(range(Dataset_Testing.shape[0])),int(Dataset_Testing.shape[0]/10) )\n",
    "    # subset 50 percents\n",
    "    Dataset_Training_sub = Dataset_Training[sub_1,:]\n",
    "    \n",
    "    Target_training_sub = Target_training[sub_1]\n",
    "    \n",
    "    Dataset_Testing_sub = Dataset_Testing[sub_2,:]\n",
    "    \n",
    "    Target_Testing_sub = Target_Testing[sub_2]\n",
    "    \n",
    "    #models, predictions = reg.fit(Dataset_Training_sub, Dataset_Testing_sub, Target_training_sub, Target_Testing_sub)\n",
    "    #print(models)\n",
    "    \n",
    "    # random forest reg\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    RF_Regmodel = RandomForestRegressor()\n",
    "    RF_Regmodel.fit(Dataset_Training_sub,Target_training_sub)\n",
    "\n",
    "\n",
    "    Predict = RF_Regmodel.predict(Dataset_Testing_sub)\n",
    "    _res = []\n",
    "    _res.append('RandomForestRegressor')\n",
    "    \n",
    "    _res.append(r2_score(Target_Testing_sub,Predict))\n",
    "    _res.append(mean_squared_error(Target_Testing_sub,Predict,squared=False))\n",
    "    res_df.append(_res)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform the validation, iteration:1\n",
      "Build Training set\n",
      "Build Testing set\n"
     ]
    }
   ],
   "source": [
    "# support vector\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "k = 0\n",
    "for train_index, test_index in KF.split(list(range(0,424))):\n",
    "    k = k +1\n",
    "    print('Perform the validation, iteration:'+str(k))\n",
    "    Targets_Train_set = Connectome_direct_density[:,train_index]\n",
    "    Targets_Train_set  = Targets_Train_set[train_index,:]\n",
    "    Targets_Test_set  = Connectome_direct_density[:,test_index]\n",
    "    Targets_Test_set   = Targets_Test_set[test_index,:]\n",
    "    \n",
    "    \n",
    "    Dataset_Train_set = Celltype_mtx_norm[train_index,:]\n",
    "    Dataset_Test_set  = Celltype_mtx_norm[test_index,:]\n",
    "    print('Build Training set')\n",
    "    Target_training = []\n",
    "    Dataset_Training = []\n",
    "    for i in range(Dataset_Train_set.shape[0]):\n",
    "        for j in range(Dataset_Train_set.shape[0]):\n",
    "            #print(i)\n",
    "            #print(j)\n",
    "            if i == j:\n",
    "                pass       \n",
    "            else:\n",
    "\n",
    "                #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "                #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "                Dataset_Training.append(np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:])))\n",
    "                Target_training.append(Targets_Train_set[i,j])\n",
    "    Dataset_Training = np.stack(Dataset_Training)\n",
    "    Target_training =np.array([np.array(xi) for xi in Target_training])   \n",
    "    \n",
    "    print('Build Testing set')\n",
    "    Target_Testing = []\n",
    "    Dataset_Testing = []\n",
    "    for i in range(Dataset_Test_set.shape[0]):\n",
    "        for j in range(Dataset_Test_set.shape[0]):\n",
    "            #print(i)\n",
    "            #print(j)\n",
    "            if i == j:\n",
    "                pass       \n",
    "            else:\n",
    "                #print(Dataset_Test_set[i,j])\n",
    "\n",
    "                #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "                #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "                Dataset_Testing.append(np.concatenate((Dataset_Test_set[i,:],Dataset_Test_set[j,:])))\n",
    "                Target_Testing.append(Targets_Test_set[i,j])\n",
    "    Dataset_Testing = np.stack(Dataset_Testing)\n",
    "    Target_Testing =np.array([np.array(xi) for xi in Target_Testing])      \n",
    "         \n",
    "    reg = LazyRegressor(verbose=1, predictions=True,ignore_warnings=False, custom_metric=None)\n",
    "    # split into 50 %\n",
    "    sub_1 = sample(list(range(Dataset_Training.shape[0])),int(Dataset_Training.shape[0]/40) )\n",
    "    sub_2 = sample(list(range(Dataset_Testing.shape[0])),int(Dataset_Testing.shape[0]/10) )\n",
    "    # subset 50 percents\n",
    "    Dataset_Training_sub = Dataset_Training[sub_1,:]\n",
    "    \n",
    "    Target_training_sub = Target_training[sub_1]\n",
    "    \n",
    "    Dataset_Testing_sub = Dataset_Testing[sub_2,:]\n",
    "    \n",
    "    Target_Testing_sub = Target_Testing[sub_2]\n",
    "    \n",
    "    #models, predictions = reg.fit(Dataset_Training_sub, Dataset_Testing_sub, Target_training_sub, Target_Testing_sub)\n",
    "    #print(models)\n",
    "    \n",
    "    # support vector regressor\n",
    "    \n",
    "    \n",
    "    svr_rbf = SVR()\n",
    "    \n",
    "    svr_rbf.fit(Dataset_Training_sub,Target_training_sub)\n",
    "\n",
    "\n",
    "    Predict = svr_rbf.predict(Dataset_Testing_sub)\n",
    "    _res = []\n",
    "    _res.append('SVR')\n",
    "    \n",
    "    _res.append(r2_score(Target_Testing_sub,Predict))\n",
    "    _res.append(mean_squared_error(Target_Testing_sub,Predict,squared=False))\n",
    "    res_df.append(_res)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform the validation, iteration:1\n",
      "Build Training set\n",
      "Build Testing set\n"
     ]
    }
   ],
   "source": [
    "# linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "k = 0\n",
    "for train_index, test_index in KF.split(list(range(0,424))):\n",
    "    k = k +1\n",
    "    print('Perform the validation, iteration:'+str(k))\n",
    "    Targets_Train_set = Connectome_direct_density[:,train_index]\n",
    "    Targets_Train_set  = Targets_Train_set[train_index,:]\n",
    "    Targets_Test_set  = Connectome_direct_density[:,test_index]\n",
    "    Targets_Test_set   = Targets_Test_set[test_index,:]\n",
    "    \n",
    "    \n",
    "    Dataset_Train_set = Celltype_mtx_norm[train_index,:]\n",
    "    Dataset_Test_set  = Celltype_mtx_norm[test_index,:]\n",
    "    print('Build Training set')\n",
    "    Target_training = []\n",
    "    Dataset_Training = []\n",
    "    for i in range(Dataset_Train_set.shape[0]):\n",
    "        for j in range(Dataset_Train_set.shape[0]):\n",
    "            #print(i)\n",
    "            #print(j)\n",
    "            if i == j:\n",
    "                pass       \n",
    "            else:\n",
    "\n",
    "                #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "                #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "                Dataset_Training.append(np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:])))\n",
    "                Target_training.append(Targets_Train_set[i,j])\n",
    "    Dataset_Training = np.stack(Dataset_Training)\n",
    "    Target_training =np.array([np.array(xi) for xi in Target_training])   \n",
    "    \n",
    "    print('Build Testing set')\n",
    "    Target_Testing = []\n",
    "    Dataset_Testing = []\n",
    "    for i in range(Dataset_Test_set.shape[0]):\n",
    "        for j in range(Dataset_Test_set.shape[0]):\n",
    "            #print(i)\n",
    "            #print(j)\n",
    "            if i == j:\n",
    "                pass       \n",
    "            else:\n",
    "                #print(Dataset_Test_set[i,j])\n",
    "\n",
    "                #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "                #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "                Dataset_Testing.append(np.concatenate((Dataset_Test_set[i,:],Dataset_Test_set[j,:])))\n",
    "                Target_Testing.append(Targets_Test_set[i,j])\n",
    "    Dataset_Testing = np.stack(Dataset_Testing)\n",
    "    Target_Testing =np.array([np.array(xi) for xi in Target_Testing])      \n",
    "         \n",
    "    reg = LazyRegressor(verbose=1, predictions=True,ignore_warnings=False, custom_metric=None)\n",
    "    # split into 50 %\n",
    "    sub_1 = sample(list(range(Dataset_Training.shape[0])),int(Dataset_Training.shape[0]/40) )\n",
    "    sub_2 = sample(list(range(Dataset_Testing.shape[0])),int(Dataset_Testing.shape[0]/10) )\n",
    "    # subset 50 percents\n",
    "    Dataset_Training_sub = Dataset_Training[sub_1,:]\n",
    "    \n",
    "    Target_training_sub = Target_training[sub_1]\n",
    "    \n",
    "    Dataset_Testing_sub = Dataset_Testing[sub_2,:]\n",
    "    \n",
    "    Target_Testing_sub = Target_Testing[sub_2]\n",
    "    \n",
    "    #models, predictions = reg.fit(Dataset_Training_sub, Dataset_Testing_sub, Target_training_sub, Target_Testing_sub)\n",
    "    #print(models)\n",
    "    \n",
    "    # support vector regressor\n",
    "    \n",
    "    \n",
    "    Lr = LinearRegression()\n",
    "    \n",
    "    Lr.fit(Dataset_Training_sub,Target_training_sub)\n",
    "\n",
    "\n",
    "    Predict = Lr.predict(Dataset_Testing_sub)\n",
    "    _res = []\n",
    "    _res.append('LinearRegression')\n",
    "    \n",
    "    _res.append(r2_score(Target_Testing_sub,Predict))\n",
    "    _res.append(mean_squared_error(Target_Testing_sub,Predict,squared=False))\n",
    "    res_df.append(_res)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "k = 0\n",
    "for train_index, test_index in KF.split(list(range(0,424))):\n",
    "    k = k +1\n",
    "    print('Perform the validation, iteration:'+str(k))\n",
    "    Targets_Train_set = Connectome_direct_density[:,train_index]\n",
    "    Targets_Train_set  = Targets_Train_set[train_index,:]\n",
    "    Targets_Test_set  = Connectome_direct_density[:,test_index]\n",
    "    Targets_Test_set   = Targets_Test_set[test_index,:]\n",
    "    \n",
    "    \n",
    "    Dataset_Train_set = Celltype_mtx_norm[train_index,:]\n",
    "    Dataset_Test_set  = Celltype_mtx_norm[test_index,:]\n",
    "    print('Build Training set')\n",
    "    Target_training = []\n",
    "    Dataset_Training = []\n",
    "    for i in range(Dataset_Train_set.shape[0]):\n",
    "        for j in range(Dataset_Train_set.shape[0]):\n",
    "            #print(i)\n",
    "            #print(j)\n",
    "            if i == j:\n",
    "                pass       \n",
    "            else:\n",
    "\n",
    "                #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "                #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "                Dataset_Training.append(np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:])))\n",
    "                Target_training.append(Targets_Train_set[i,j])\n",
    "    Dataset_Training = np.stack(Dataset_Training)\n",
    "    Target_training =np.array([np.array(xi) for xi in Target_training])   \n",
    "    \n",
    "    print('Build Testing set')\n",
    "    Target_Testing = []\n",
    "    Dataset_Testing = []\n",
    "    for i in range(Dataset_Test_set.shape[0]):\n",
    "        for j in range(Dataset_Test_set.shape[0]):\n",
    "            #print(i)\n",
    "            #print(j)\n",
    "            if i == j:\n",
    "                pass       \n",
    "            else:\n",
    "                #print(Dataset_Test_set[i,j])\n",
    "\n",
    "                #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "                #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "                Dataset_Testing.append(np.concatenate((Dataset_Test_set[i,:],Dataset_Test_set[j,:])))\n",
    "                Target_Testing.append(Targets_Test_set[i,j])\n",
    "    Dataset_Testing = np.stack(Dataset_Testing)\n",
    "    Target_Testing =np.array([np.array(xi) for xi in Target_Testing])      \n",
    "         \n",
    "    reg = LazyRegressor(verbose=1, predictions=True,ignore_warnings=False, custom_metric=None)\n",
    "    # split into 50 %\n",
    "    sub_1 = sample(list(range(Dataset_Training.shape[0])),int(Dataset_Training.shape[0]/40) )\n",
    "    sub_2 = sample(list(range(Dataset_Testing.shape[0])),int(Dataset_Testing.shape[0]/10) )\n",
    "    # subset 50 percents\n",
    "    Dataset_Training_sub = Dataset_Training[sub_1,:]\n",
    "    \n",
    "    Target_training_sub = Target_training[sub_1]\n",
    "    \n",
    "    Dataset_Testing_sub = Dataset_Testing[sub_2,:]\n",
    "    \n",
    "    Target_Testing_sub = Target_Testing[sub_2]\n",
    "    \n",
    "    #models, predictions = reg.fit(Dataset_Training_sub, Dataset_Testing_sub, Target_training_sub, Target_Testing_sub)\n",
    "    #print(models)\n",
    "    \n",
    "    # support vector regressor\n",
    "    \n",
    "    \n",
    "    Lr = LinearRegression()\n",
    "    \n",
    "    Lr.fit(Dataset_Training_sub,Target_training_sub)\n",
    "\n",
    "\n",
    "    Predict = Lr.predict(Dataset_Testing_sub)\n",
    "    _res = []\n",
    "    _res.append('LinearRegression')\n",
    "    \n",
    "    _res.append(r2_score(Target_Testing_sub,Predict))\n",
    "    _res.append(mean_squared_error(Target_Testing_sub,Predict,squared=False))\n",
    "    res_df.append(_res)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Methods</th>\n",
       "      <th>R_square</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>-2.17</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVR</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>1.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Methods  R_square  RMSE\n",
       "0  RandomForestRegressor     -2.17  2.75\n",
       "1                    SVR     -0.01  2.75\n",
       "2       LinearRegression     -0.09  1.73"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Result = pd.DataFrame(res_df,columns = ['Methods','R_square','RMSE'])\n",
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_Testing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\n",
    "models, predictions = reg.fit(Dataset_Training, Dataset_Testing, Target_training, Target_Testing)\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
