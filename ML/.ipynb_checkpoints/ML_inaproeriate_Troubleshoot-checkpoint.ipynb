{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jingjinglab/cw/miniconda3/envs/Harry_env/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import lazypredict\n",
    "import mat4py\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import os \n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## \n",
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "Connectomes = scipy.io.loadmat('../Cells2Connectomes/Connectomes.mat')\n",
    "Connectome_direct = Connectomes['C_dir']\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Region volumes, in a 424 vector, to get connectivity density, divide\n",
    "% each row in connectomes by each entry in the vector to get density. Units\n",
    "% are in 200 micron per vertex voxels.\n",
    "\n",
    "'''\n",
    "\n",
    "CellType_volumn = mat4py.loadmat('../Cells2Connectomes/Regional_Volumes.mat')\n",
    "CellType_volumn = CellType_volumn['region_vols']\n",
    "Celltype_volumn =np.array([np.array(xi) for xi in CellType_volumn])\n",
    "Celltype_volumn.shape\n",
    "\n",
    "# Nomarlize by the entry\n",
    "\n",
    "Connectome_direct_density = Connectome_direct/Celltype_volumn\n",
    "\n",
    "Cell_type = mat4py.loadmat('../Cells2Connectomes/CellType_Maps.mat')\n",
    "Cell_type = Cell_type['cellmaps']\n",
    "Celltype_mtx =np.array([np.array(xi) for xi in Cell_type])\n",
    "Celltype_mtx.shape\n",
    "\n",
    "# Important : normalizing via the columns\n",
    "\n",
    "Celltype_mtx_norm = (Celltype_mtx.max(axis=0)-Celltype_mtx) / (Celltype_mtx.max(axis=0) - Celltype_mtx.min(axis=0) )\n",
    "\n",
    "\n",
    "# region names\n",
    "Region_maps = mat4py.loadmat('../Cells2Connectomes/Region_Names.mat')\n",
    "Region_maps = Region_maps['region_names']\n",
    "Regionmaps_df = pd.DataFrame(Region_maps,columns = ['Anno1','Anno2','Anno3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'region_names': [['Anterior amygdalar area',\n",
       "   'Basolat amygdalar nucleus',\n",
       "   'Basomed amygdalar nucleus'],\n",
       "  ['Central amygdalar nucleus',\n",
       "   'Cortical amygdalar area, anterior part',\n",
       "   'Cortical amygdalar area, posterior part'],\n",
       "  ['Intercalated amygdalar nucleus',\n",
       "   'Lateral amygdalar nucleus',\n",
       "   'Medial amygdalar nucleus'],\n",
       "  ['Posterior amygdalar nucleus', 'Piriform-amygdalar area', 'Central lobule'],\n",
       "  ['Culmen', 'Flocculus area', 'Nodulus (X)'],\n",
       "  ['Paraflocculus', 'Paramedian lobule', 'Pyramus (VIII)'],\n",
       "  ['Simple lobule', 'Dentate nucleus', 'Fastigial nucleus'],\n",
       "  ['Interposed nucleus', 'Claustrum', 'Endopiriform nucleus, dorsal part'],\n",
       "  ['Endopiriform nucleus, ventral part', 'Dentate gyrus', 'Field CA3'],\n",
       "  ['Field CA2', 'Field CA1', 'Subiculum, dorsal part'],\n",
       "  ['Subiculum, ventral part', 'Presubiculum', 'Parasubiculum'],\n",
       "  ['Postsubiculum',\n",
       "   'Entorhinal area, lateral part',\n",
       "   'Entorhinal area, medial part, dorsal zone'],\n",
       "  ['Anterior hypo nucleus',\n",
       "   'Arcuate hypo nucleus',\n",
       "   'Dorsomedial nucleus of the hypo'],\n",
       "  ['Lateral hypo area', 'Lateral preoptic area', 'Median preoptic nucleus'],\n",
       "  ['Medial mammillary nucleus',\n",
       "   'Medial preoptic nucleus',\n",
       "   'Medial preoptic area'],\n",
       "  ['Posterior hypo nucleus',\n",
       "   'Dorsal premammillary nucleus',\n",
       "   'Paraventricular hypo nucleus'],\n",
       "  ['Periventricular hypo nucleus, posterior part',\n",
       "   'Periventricular hypo nucleus, preoptic part',\n",
       "   'Retrochiasmatic area'],\n",
       "  ['Subparaventricular zone', 'Subthalam nucleus', 'Supramammillary nucleus'],\n",
       "  ['Tuberal nucleus',\n",
       "   'Ventromedial hypo nucleus',\n",
       "   'Anterior cingulate area, dorsal part'],\n",
       "  ['Anterior cingulate area, ventral part',\n",
       "   'Agranular insular area, dorsal part',\n",
       "   'Agranular insular area, posterior part'],\n",
       "  ['Agranular insular area, ventral part',\n",
       "   'Dorsal auditory area',\n",
       "   'Primary auditory area'],\n",
       "  ['Ventral auditory area',\n",
       "   'Ectorhinal area',\n",
       "   'Frontal pole, cerebral cortex'],\n",
       "  ['Gustatory areas', 'Infralimbic area', 'Primary motor area'],\n",
       "  ['Secondary motor area',\n",
       "   'Orbital area, lateral part',\n",
       "   'Orbital area, medial part'],\n",
       "  ['Orbital area, ventrolateral part', 'Perirhinal area', 'Prelimbic area'],\n",
       "  ['Posterior parietal association areas',\n",
       "   'Retrosplenial area, lateral agranular part',\n",
       "   'Retrosplenial area, dorsal part'],\n",
       "  ['Retrosplenial area, ventral part',\n",
       "   'Primary somatosensory area, barrel field',\n",
       "   'Primary somatosensory area, lower limb'],\n",
       "  ['Primary somatosensory area, mouth',\n",
       "   'Primary somatosensory area, nose',\n",
       "   'Primary somatosensory area, trunk'],\n",
       "  ['Primary somatosensory area, upper limb',\n",
       "   'Supplemental somatosensory area',\n",
       "   'Temporal association areas'],\n",
       "  ['Anterolat visual area', 'Anteromedial visual area', 'Visceral area'],\n",
       "  ['Lateral visual area', 'Primary visual area', 'Posterolat visual area'],\n",
       "  ['posteromedial visual area', 'Nucleus ambiguus', 'Dorsal cochlear nucleus'],\n",
       "  ['Gigantocellular reticular nucleus',\n",
       "   'Inferior olivary complex',\n",
       "   'Intermediate reticular nucleus'],\n",
       "  ['Lateral vestibular nucleus',\n",
       "   'Lateral reticular nucleus',\n",
       "   'Magnocellular reticular nucleus'],\n",
       "  ['Medullary reticular nucleus, dorsal part',\n",
       "   'Medullary reticular nucleus, ventral part',\n",
       "   'Medial vestibular nucleus'],\n",
       "  ['Nucleus of the solitary tract',\n",
       "   'Parvicellular reticular nucleus',\n",
       "   'Paragigantocellular reticular nucleus, dorsal part'],\n",
       "  ['Paragigantocellular reticular nucleus, lateral part',\n",
       "   'Nucleus prepositus',\n",
       "   'Nucleus raphe magnus'],\n",
       "  ['Spinal vestibular nucleus',\n",
       "   'Spinal nucleus of the trigeminal, caudal part',\n",
       "   'Spinal nucleus of the trigeminal, interpolar part'],\n",
       "  ['Spinal nucleus of the trigeminal, oral part',\n",
       "   'Superior vestibular nucleus',\n",
       "   'Ventral cochlear nucleus'],\n",
       "  ['Facial motor nucleus',\n",
       "   'Hypoglossal nucleus',\n",
       "   'Anterior pretectal nucleus'],\n",
       "  ['Central linear nucleus raphe',\n",
       "   'Cuneiform nucleus',\n",
       "   'Dorsal nucleus raphe'],\n",
       "  ['Inferior colliculus, central nucleus',\n",
       "   'Inferior colliculus, dorsal nucleus',\n",
       "   'Inferior colliculus, external nucleus'],\n",
       "  ['Interpeduncular nucleus',\n",
       "   'Medial pretectal area',\n",
       "   'Midbrain reticular nucleus'],\n",
       "  ['Nucleus of the optic tract',\n",
       "   'Nucleus of the posterior commissure',\n",
       "   'Periaqueductal gray'],\n",
       "  ['Pedunculopontine nucleus',\n",
       "   'Red nucleus',\n",
       "   'Midbrain reticular nucleus, retrorubral area'],\n",
       "  ['Superior colliculus, motor related',\n",
       "   'Superior colliculus, sensory related',\n",
       "   'Substantia nigra, compact part'],\n",
       "  ['Substantia nigra, reticular part',\n",
       "   'Ventral tegmental area',\n",
       "   'Accessory olfactory bulb'],\n",
       "  ['Anterior olfactory nucleus',\n",
       "   'Dorsal peduncular area',\n",
       "   'Main olfactory bulb'],\n",
       "  ['Nucleus of the lateral olfactory tract',\n",
       "   'Piriform area',\n",
       "   'Postpiriform transition area'],\n",
       "  ['Taenia tecta',\n",
       "   'Bed nuclei of the stria terminalis',\n",
       "   'Globus pallidus, external segment'],\n",
       "  ['Globus pallidus, internal segment',\n",
       "   'Magnocellular nucleus',\n",
       "   'Medial septal nucleus'],\n",
       "  ['Diagonal band nucleus',\n",
       "   'Substantia innominata',\n",
       "   'Triangular nucleus of septum'],\n",
       "  ['Superior central nucleus raphe',\n",
       "   'Nucleus incertus',\n",
       "   'Nucleus of the lateral lemniscus'],\n",
       "  ['Parabrachial nucleus', 'Pontine central gray', 'Pontine gray'],\n",
       "  ['Pontine reticular nucleus, caudal part',\n",
       "   'Pontine reticular nucleus',\n",
       "   'Principal sensory nucleus of the trigeminal'],\n",
       "  ['Superior olivary complex',\n",
       "   'Supratrigeminal nucleus',\n",
       "   'Tegmental reticular nucleus'],\n",
       "  ['Motor nucleus of trigeminal', 'Nucleus accumbens', 'Caudoputamen'],\n",
       "  ['Fundus',\n",
       "   'Lateral septal nucleus, caudal (caudodorsal) part',\n",
       "   'Lateral septal nucleus, rostral (rostroventral) part'],\n",
       "  ['Lateral septal nucleus, ventral part',\n",
       "   'Olfactory tubercle',\n",
       "   'Septofimbrial nucleus'],\n",
       "  ['Anterodorsal nucleus',\n",
       "   'Anteromedial nucleus, dorsal part',\n",
       "   'Anteromedial nucleus, ventral part'],\n",
       "  ['Anteroventral nucleus of thalamus',\n",
       "   'Central lateral nucleus of the thalamus',\n",
       "   'Central medial nucleus of the thalamus'],\n",
       "  ['Intermediodorsal nucleus of the thalamus',\n",
       "   'Lateral dorsal nucleus thalamus',\n",
       "   'Dorsal part of the lateral geniculate complex'],\n",
       "  ['Ventral part of the lateral geniculate complex',\n",
       "   'Lateral habenula',\n",
       "   'Lateral posterior nucleus of the thalamus'],\n",
       "  ['Mediodorsal nucleus of thalamus',\n",
       "   'Medial geniculate complex, dorsal part',\n",
       "   'Medial geniculate complex, medial part'],\n",
       "  ['Medial geniculate complex, ventral part',\n",
       "   'Medial habenula',\n",
       "   'Parafascicular nucleus'],\n",
       "  ['Posterior complex of the thalamus',\n",
       "   'Posterior limiting nucleus of the thalamus',\n",
       "   'Peripeduncular nucleus'],\n",
       "  ['Parataenial nucleus',\n",
       "   'Paraventricular nucleus of the thalamus',\n",
       "   'Nucleus of reunions'],\n",
       "  ['Rhomboid nucleus',\n",
       "   'Reticular nucleus of the thalamus',\n",
       "   'Submedial nucleus of the thalamus'],\n",
       "  ['Subparafascicular area',\n",
       "   'Subparafascicular nucleus, magnocellular part',\n",
       "   'Subparafascicular nucleus, parvicellular part'],\n",
       "  ['Ventral anterior-lateral complex of the thalamus',\n",
       "   'Ventral nucleus thalamus medial',\n",
       "   'Ventral nucleus thalamus posterolateral'],\n",
       "  ['Ventral nucleus thalamus',\n",
       "   'Ventral nucleus thalamus posteromedial parvicellular part',\n",
       "   'Amygdala'],\n",
       "  ['Amygdala', 'Amygdala', 'Amygdala'],\n",
       "  ['Amygdala', 'Amygdala', 'Amygdala'],\n",
       "  ['Amygdala', 'Amygdala', 'Amygdala'],\n",
       "  ['Amygdala', 'Cerebellar Cortex', 'Cerebellar Cortex'],\n",
       "  ['Cerebellar Cortex', 'Cerebellar Cortex', 'Cerebellar Cortex'],\n",
       "  ['Cerebellar Cortex', 'Cerebellar Cortex', 'Cerebellar Cortex'],\n",
       "  ['Cerebellar Nuclei', 'Cerebellar Nuclei', 'Cerebellar Nuclei'],\n",
       "  ['Cortical Subplate', 'Cortical Subplate', 'Cortical Subplate'],\n",
       "  ['Hippocampal Formation', 'Hippocampal Formation', 'Hippocampal Formation'],\n",
       "  ['Hippocampal Formation', 'Hippocampal Formation', 'Hippocampal Formation'],\n",
       "  ['Hippocampal Formation', 'Hippocampal Formation', 'Hippocampal Formation'],\n",
       "  ['Hippocampal Gate', 'Hippocampal Gate', 'Hypothalamus'],\n",
       "  ['Hypothalamus', 'Hypothalamus', 'Hypothalamus'],\n",
       "  ['Hypothalamus', 'Hypothalamus', 'Hypothalamus'],\n",
       "  ['Hypothalamus', 'Hypothalamus', 'Hypothalamus'],\n",
       "  ['Hypothalamus', 'Hypothalamus', 'Hypothalamus'],\n",
       "  ['Hypothalamus', 'Hypothalamus', 'Hypothalamus'],\n",
       "  ['Hypothalamus', 'Hypothalamus', 'Hypothalamus'],\n",
       "  ['Hypothalamus', 'Isocortex', 'Isocortex'],\n",
       "  ['Isocortex', 'Isocortex', 'Isocortex'],\n",
       "  ['Isocortex', 'Isocortex', 'Isocortex'],\n",
       "  ['Isocortex', 'Isocortex', 'Isocortex'],\n",
       "  ['Isocortex', 'Isocortex', 'Isocortex'],\n",
       "  ['Isocortex', 'Isocortex', 'Isocortex'],\n",
       "  ['Isocortex', 'Isocortex', 'Isocortex'],\n",
       "  ['Isocortex', 'Isocortex', 'Isocortex'],\n",
       "  ['Isocortex', 'Isocortex', 'Isocortex'],\n",
       "  ['Isocortex', 'Isocortex', 'Isocortex'],\n",
       "  ['Isocortex', 'Isocortex', 'Isocortex'],\n",
       "  ['Isocortex', 'Isocortex', 'Isocortex'],\n",
       "  ['Isocortex', 'Isocortex', 'Isocortex'],\n",
       "  ['Medulla', 'Medulla', 'Medulla'],\n",
       "  ['Medulla', 'Medulla', 'Medulla'],\n",
       "  ['Medulla', 'Medulla', 'Medulla'],\n",
       "  ['Medulla', 'Medulla', 'Medulla'],\n",
       "  ['Medulla', 'Medulla', 'Medulla'],\n",
       "  ['Medulla', 'Medulla', 'Medulla'],\n",
       "  ['Medulla', 'Medulla', 'Medulla'],\n",
       "  ['Medulla', 'Medulla', 'Medulla'],\n",
       "  ['Medulla', 'Midbrain', 'Midbrain'],\n",
       "  ['Midbrain', 'Midbrain', 'Midbrain'],\n",
       "  ['Midbrain', 'Midbrain', 'Midbrain'],\n",
       "  ['Midbrain', 'Midbrain', 'Midbrain'],\n",
       "  ['Midbrain', 'Midbrain', 'Midbrain'],\n",
       "  ['Midbrain', 'Midbrain', 'Midbrain'],\n",
       "  ['Midbrain', 'Midbrain', 'Midbrain'],\n",
       "  ['Midbrain', 'Olfactory Areas', 'Olfactory Areas'],\n",
       "  ['Olfactory Areas', 'Olfactory Areas', 'Olfactory Areas'],\n",
       "  ['Olfactory Areas', 'Olfactory Areas', 'Olfactory Areas'],\n",
       "  ['Pallidum', 'Pallidum', 'Pallidum'],\n",
       "  ['Pallidum', 'Pallidum', 'Pallidum'],\n",
       "  ['Pallidum', 'Pallidum', 'Pons'],\n",
       "  ['Pons', 'Pons', 'Pons'],\n",
       "  ['Pons', 'Pons', 'Pons'],\n",
       "  ['Pons', 'Pons', 'Pons'],\n",
       "  ['Pons', 'Pons', 'Pons'],\n",
       "  ['Striatum', 'Striatum', 'Striatum'],\n",
       "  ['Striatum', 'Striatum', 'Striatum'],\n",
       "  ['Striatum', 'Striatum', 'Thalamic'],\n",
       "  ['Thalamic', 'Thalamic', 'Thalamic'],\n",
       "  ['Thalamic', 'Thalamic', 'Thalamic'],\n",
       "  ['Thalamic', 'Thalamic', 'Thalamic'],\n",
       "  ['Thalamic', 'Thalamic', 'Thalamic'],\n",
       "  ['Thalamic', 'Thalamic', 'Thalamic'],\n",
       "  ['Thalamic', 'Thalamic', 'Thalamic'],\n",
       "  ['Thalamic', 'Thalamic', 'Thalamic'],\n",
       "  ['Thalamic', 'Thalamic', 'Thalamic'],\n",
       "  ['Thalamic', 'Thalamic', 'Thalamic'],\n",
       "  ['Thalamic', 'Thalamic', 'Thalamic'],\n",
       "  ['Thalamic', 'Thalamic', 'Thalamic'],\n",
       "  ['Thalamic', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Hippocampus'],\n",
       "  ['Hippocampus', 'Hippocampus', 'Hippocampus'],\n",
       "  ['Hippocampus', 'Hippocampus', 'Hippocampus'],\n",
       "  ['Hippocampus', 'Hippocampus', 'Hippocampus'],\n",
       "  ['Hippocampus', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Cingulate Area', 'Cingulate Area', 'Insula'],\n",
       "  ['Insula', 'Insula', 'Auditory Cortex'],\n",
       "  ['Auditory Cortex', 'Auditory Cortex', 'Parahippocampus'],\n",
       "  ['Frontal Cortex', 'Gustatory Cortex', 'Limbic Area'],\n",
       "  ['Motor Cortex', 'Motor Cortex', 'Frontal Cortex'],\n",
       "  ['Frontal Cortex', 'Frontal Cortex', 'Parahippocampus'],\n",
       "  ['Limbic Area', 'Auditory Cortex', 'Retrosplenial Area'],\n",
       "  ['Retrosplenial Area', 'Retrosplenial Area', 'Somatosensory Cortex'],\n",
       "  ['Somatosensory Cortex', 'Somatosensory Cortex', 'Somatosensory Cortex'],\n",
       "  ['Somatosensory Cortex', 'Somatosensory Cortex', 'Somatosensory Cortex'],\n",
       "  ['Visual Cortex', 'Visual Cortex', 'Visual Cortex'],\n",
       "  ['Visual Cortex', 'Visual Cortex', 'Visual Cortex'],\n",
       "  ['Visual Cortex', 'Visual Cortex', 'Brainstem'],\n",
       "  ['Brainstem', 'Brainstem', 'Brainstem'],\n",
       "  ['Brainstem', 'Brainstem', 'Brainstem'],\n",
       "  ['Brainstem', 'Brainstem', 'Brainstem'],\n",
       "  ['Brainstem', 'Brainstem', 'Brainstem'],\n",
       "  ['Brainstem', 'Brainstem', 'Brainstem'],\n",
       "  ['Brainstem', 'Brainstem', 'Brainstem'],\n",
       "  ['Brainstem', 'Brainstem', 'Brainstem'],\n",
       "  ['Brainstem', 'Brainstem', 'Brainstem'],\n",
       "  ['Brainstem', 'Brainstem', 'Brainstem'],\n",
       "  ['Brainstem', 'Brainstem', 'Brainstem'],\n",
       "  ['Brainstem', 'Brainstem', 'Brainstem'],\n",
       "  ['Brainstem', 'Brainstem', 'Brainstem'],\n",
       "  ['Brainstem', 'Brainstem', 'Brainstem'],\n",
       "  ['Brainstem', 'Brainstem', 'Brainstem'],\n",
       "  ['Brainstem', 'Brainstem', 'Brainstem'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Brainstem', 'Brainstem'],\n",
       "  ['Brainstem', 'Brainstem', 'Brainstem'],\n",
       "  ['Brainstem', 'Brainstem', 'Brainstem'],\n",
       "  ['Brainstem', 'Brainstem', 'Brainstem'],\n",
       "  ['Brainstem', 'Brainstem', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter'],\n",
       "  ['Gray Matter', 'Gray Matter', 'Gray Matter']]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Region_maps = mat4py.loadmat('../Cells2Connectomes/Region_Names.mat')\n",
    "Region_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anno1</th>\n",
       "      <th>Anno2</th>\n",
       "      <th>Anno3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Visual Cortex</td>\n",
       "      <td>Visual Cortex</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Gray Matter</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "      <td>Brainstem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Anno1          Anno2      Anno3\n",
       "172  Visual Cortex  Visual Cortex  Brainstem\n",
       "173      Brainstem      Brainstem  Brainstem\n",
       "174      Brainstem      Brainstem  Brainstem\n",
       "175      Brainstem      Brainstem  Brainstem\n",
       "176      Brainstem      Brainstem  Brainstem\n",
       "177      Brainstem      Brainstem  Brainstem\n",
       "178      Brainstem      Brainstem  Brainstem\n",
       "179      Brainstem      Brainstem  Brainstem\n",
       "180      Brainstem      Brainstem  Brainstem\n",
       "181      Brainstem      Brainstem  Brainstem\n",
       "182      Brainstem      Brainstem  Brainstem\n",
       "183      Brainstem      Brainstem  Brainstem\n",
       "184      Brainstem      Brainstem  Brainstem\n",
       "185      Brainstem      Brainstem  Brainstem\n",
       "186      Brainstem      Brainstem  Brainstem\n",
       "187      Brainstem      Brainstem  Brainstem\n",
       "193    Gray Matter      Brainstem  Brainstem\n",
       "194      Brainstem      Brainstem  Brainstem\n",
       "195      Brainstem      Brainstem  Brainstem\n",
       "196      Brainstem      Brainstem  Brainstem"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Regionmaps_df[Regionmaps_df['Anno3'] =='Brainstem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(Regionmaps_df['Anno1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KF = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "KF.get_n_splits(list(range(0,424)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/42 [00:01<00:41,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'AdaBoostRegressor', 'R-Squared': -0.3786686560687367, 'Adjusted R-Squared': -0.48217230892675, 'RMSE': 3.1561528071856246, 'Time taken': 1.0213963985443115}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▍         | 2/42 [00:02<01:02,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'BaggingRegressor', 'R-Squared': -0.32052449293821494, 'Adjusted R-Squared': -0.41966296838402695, 'RMSE': 3.0888818804776794, 'Time taken': 1.954134464263916}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 3/42 [00:03<00:43,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'BayesianRidge', 'R-Squared': 0.010090293557922125, 'Adjusted R-Squared': -0.06422725197076229, 'RMSE': 2.6743969113471504, 'Time taken': 0.5914475917816162}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 6/42 [00:04<00:15,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'DecisionTreeRegressor', 'R-Squared': -1.3823158892022693, 'Adjusted R-Squared': -1.561168433436674, 'RMSE': 4.148850104804744, 'Time taken': 0.34850454330444336}\n",
      "{'Model': 'DummyRegressor', 'R-Squared': -0.0008488227489387157, 'Adjusted R-Squared': -0.07598762325561581, 'RMSE': 2.689133184374062, 'Time taken': 0.019545316696166992}\n",
      "{'Model': 'ElasticNet', 'R-Squared': -0.0008488227489387157, 'Adjusted R-Squared': -0.07598762325561581, 'RMSE': 2.689133184374062, 'Time taken': 0.10327410697937012}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 8/42 [00:04<00:10,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'ElasticNetCV', 'R-Squared': -0.0008488227489387157, 'Adjusted R-Squared': -0.07598762325561581, 'RMSE': 2.689133184374062, 'Time taken': 0.3464369773864746}\n",
      "{'Model': 'ExtraTreeRegressor', 'R-Squared': -1.0807405500968006, 'Adjusted R-Squared': -1.236952303107071, 'RMSE': 3.8773681284549175, 'Time taken': 0.11194920539855957}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██▏       | 9/42 [00:08<00:42,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'ExtraTreesRegressor', 'R-Squared': -0.02139274368784383, 'Adjusted R-Squared': -0.0980738806013457, 'RMSE': 2.7165922335166908, 'Time taken': 3.7173540592193604}\n",
      "GammaRegressor model failed to execute\n",
      "Some value(s) of y are out of the valid range for family GammaDistribution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 11/42 [00:12<00:51,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'GaussianProcessRegressor', 'R-Squared': -0.02357508807421871, 'Adjusted R-Squared': -0.10042006465636732, 'RMSE': 2.719492869165123, 'Time taken': 4.231747627258301}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▊       | 12/42 [00:12<00:40,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'GeneralizedLinearRegressor', 'R-Squared': 0.011124759162714715, 'Adjusted R-Squared': -0.06311512378302742, 'RMSE': 2.6729991602431666, 'Time taken': 0.31197547912597656}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 13/42 [00:16<00:56,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'GradientBoostingRegressor', 'R-Squared': -0.4775685000010077, 'Adjusted R-Squared': -0.5884970660671494, 'RMSE': 3.267396824388825, 'Time taken': 3.7326996326446533}\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "\n",
    "# build a df to store the information\n",
    "\n",
    "# instead use the subset, this time I am trying the whole, which might be not suitable for ML, but I just wanted a positive control\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "res_df =[]\n",
    "\n",
    "\n",
    "Target = []\n",
    "Dataset = []\n",
    "for i in range(Celltype_mtx_norm.shape[0]):\n",
    "    for j in range(Celltype_mtx_norm.shape[0]):\n",
    "        #print(i)\n",
    "        #print(j)\n",
    "        if i == j:\n",
    "            pass       \n",
    "        else:\n",
    "\n",
    "            #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "            #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "            Dataset.append(np.concatenate((Celltype_mtx_norm[i,:],Celltype_mtx_norm[j,:],\n",
    "                                          )))\n",
    "            Target.append(Connectome_direct_density[i,j])\n",
    "Dataset = np.stack(Dataset)\n",
    "Target =np.array([np.array(xi) for xi in Target])   \n",
    "    \n",
    "#X_train, X_test, y_train, y_test = train_test_split(Dataset, Target,test_size=.2,random_state =123)  \n",
    "\n",
    "#\n",
    "kf = KFold(n_splits=5, shuffle= True)\n",
    "\n",
    "for train_index, test_index in kf.split(Dataset):\n",
    "    sub1 = sample(list(train_index), int(len(train_index)/50))\n",
    "    sub2 = sample(list(test_index), int(len(test_index)/50))\n",
    "    X_train, X_test = Dataset[sub1], Dataset[sub2]\n",
    "    y_train, y_test = Target[sub1], Target[sub2]\n",
    "  \n",
    "    \n",
    "    \n",
    "    reg = LazyRegressor(verbose=1, predictions=True,ignore_warnings=False, custom_metric=None)\n",
    "    models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
    "    print(models)\n",
    "    models.to_csv('res_inproper.csv', sep = '\\t')\n",
    "    break\n",
    "# split into 50 %\n",
    "#sub_1 = sample(list(range(X_train.shape[0])),int(X_train.shape[0]/10) )\n",
    "#sub_2 = sample(list(range(X_test.shape[0])),int(X_test.shape[0]/10) )\n",
    "# subset 50 percents\n",
    "#Dataset_Training_sub = X_train[sub_1,:]\n",
    "    \n",
    "#Target_training_sub = y_train[sub_1]\n",
    "    \n",
    "#Dataset_Testing_sub = X_test[sub_2,:]\n",
    "    \n",
    "#Target_Testing_sub = y_test[sub_2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adjusted R-Squared</th>\n",
       "      <th>R-Squared</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GaussianProcessRegressor</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.17</td>\n",
       "      <td>81.72</td>\n",
       "      <td>6.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoLars</th>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>90.38</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyRegressor</th>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>90.38</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSVR</th>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>91.47</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>91.51</td>\n",
       "      <td>1.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LarsCV</th>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>91.59</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoLarsCV</th>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>91.61</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HuberRegressor</th>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>91.61</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVR</th>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>91.62</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNetCV</th>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>91.76</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoCV</th>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>91.92</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveRegressor</th>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>92.20</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TweedieRegressor</th>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>92.79</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GeneralizedLinearRegressor</th>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>92.79</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OrthogonalMatchingPursuitCV</th>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>92.80</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OrthogonalMatchingPursuit</th>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>92.80</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BayesianRidge</th>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>92.81</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNet</th>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>93.32</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LassoLarsIC</th>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>94.15</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>95.43</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeCV</th>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>97.50</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>97.59</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransformedTargetRegressor</th>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>97.60</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearRegression</th>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>97.60</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KernelRidge</th>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>98.98</td>\n",
       "      <td>11.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lars</th>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>100.18</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDRegressor</th>\n",
       "      <td>-0.36</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>101.14</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoissonRegressor</th>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>102.76</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPRegressor</th>\n",
       "      <td>-0.42</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>103.23</td>\n",
       "      <td>27.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HistGradientBoostingRegressor</th>\n",
       "      <td>-0.70</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>112.89</td>\n",
       "      <td>526.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <td>-0.73</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>113.92</td>\n",
       "      <td>55.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesRegressor</th>\n",
       "      <td>-0.90</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>119.50</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <td>-1.02</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>123.15</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsRegressor</th>\n",
       "      <td>-1.07</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>124.55</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBRegressor</th>\n",
       "      <td>-1.26</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>130.33</td>\n",
       "      <td>63.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingRegressor</th>\n",
       "      <td>-1.46</td>\n",
       "      <td>-1.28</td>\n",
       "      <td>135.79</td>\n",
       "      <td>1.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>-2.10</td>\n",
       "      <td>-1.88</td>\n",
       "      <td>152.47</td>\n",
       "      <td>3.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeRegressor</th>\n",
       "      <td>-5.17</td>\n",
       "      <td>-4.74</td>\n",
       "      <td>215.20</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeRegressor</th>\n",
       "      <td>-6.50</td>\n",
       "      <td>-5.98</td>\n",
       "      <td>237.32</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostRegressor</th>\n",
       "      <td>-10.13</td>\n",
       "      <td>-9.35</td>\n",
       "      <td>289.08</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Adjusted R-Squared  R-Squared   RMSE  \\\n",
       "Model                                                                 \n",
       "GaussianProcessRegressor                     0.11       0.17  81.72   \n",
       "LassoLars                                   -0.09      -0.01  90.38   \n",
       "DummyRegressor                              -0.09      -0.01  90.38   \n",
       "NuSVR                                       -0.11      -0.04  91.47   \n",
       "SVR                                         -0.12      -0.04  91.51   \n",
       "LarsCV                                      -0.12      -0.04  91.59   \n",
       "LassoLarsCV                                 -0.12      -0.04  91.61   \n",
       "HuberRegressor                              -0.12      -0.04  91.61   \n",
       "LinearSVR                                   -0.12      -0.04  91.62   \n",
       "ElasticNetCV                                -0.12      -0.04  91.76   \n",
       "LassoCV                                     -0.13      -0.05  91.92   \n",
       "PassiveAggressiveRegressor                  -0.13      -0.05  92.20   \n",
       "TweedieRegressor                            -0.15      -0.07  92.79   \n",
       "GeneralizedLinearRegressor                  -0.15      -0.07  92.79   \n",
       "OrthogonalMatchingPursuitCV                 -0.15      -0.07  92.80   \n",
       "OrthogonalMatchingPursuit                   -0.15      -0.07  92.80   \n",
       "BayesianRidge                               -0.15      -0.07  92.81   \n",
       "ElasticNet                                  -0.16      -0.08  93.32   \n",
       "LassoLarsIC                                 -0.18      -0.10  94.15   \n",
       "Lasso                                       -0.21      -0.13  95.43   \n",
       "RidgeCV                                     -0.27      -0.18  97.50   \n",
       "Ridge                                       -0.27      -0.18  97.59   \n",
       "TransformedTargetRegressor                  -0.27      -0.18  97.60   \n",
       "LinearRegression                            -0.27      -0.18  97.60   \n",
       "KernelRidge                                 -0.30      -0.21  98.98   \n",
       "Lars                                        -0.34      -0.24 100.18   \n",
       "SGDRegressor                                -0.36      -0.27 101.14   \n",
       "PoissonRegressor                            -0.41      -0.31 102.76   \n",
       "MLPRegressor                                -0.42      -0.32 103.23   \n",
       "HistGradientBoostingRegressor               -0.70      -0.58 112.89   \n",
       "LGBMRegressor                               -0.73      -0.61 113.92   \n",
       "ExtraTreesRegressor                         -0.90      -0.77 119.50   \n",
       "RandomForestRegressor                       -1.02      -0.88 123.15   \n",
       "KNeighborsRegressor                         -1.07      -0.92 124.55   \n",
       "XGBRegressor                                -1.26      -1.10 130.33   \n",
       "BaggingRegressor                            -1.46      -1.28 135.79   \n",
       "GradientBoostingRegressor                   -2.10      -1.88 152.47   \n",
       "ExtraTreeRegressor                          -5.17      -4.74 215.20   \n",
       "DecisionTreeRegressor                       -6.50      -5.98 237.32   \n",
       "AdaBoostRegressor                          -10.13      -9.35 289.08   \n",
       "\n",
       "                               Time Taken  \n",
       "Model                                      \n",
       "GaussianProcessRegressor             6.75  \n",
       "LassoLars                            0.05  \n",
       "DummyRegressor                       0.02  \n",
       "NuSVR                                1.06  \n",
       "SVR                                  1.12  \n",
       "LarsCV                               0.28  \n",
       "LassoLarsCV                          0.26  \n",
       "HuberRegressor                       1.18  \n",
       "LinearSVR                            0.52  \n",
       "ElasticNetCV                         0.30  \n",
       "LassoCV                              0.25  \n",
       "PassiveAggressiveRegressor           0.06  \n",
       "TweedieRegressor                     0.23  \n",
       "GeneralizedLinearRegressor           0.33  \n",
       "OrthogonalMatchingPursuitCV          0.11  \n",
       "OrthogonalMatchingPursuit            0.04  \n",
       "BayesianRidge                        0.39  \n",
       "ElasticNet                           0.07  \n",
       "LassoLarsIC                          0.06  \n",
       "Lasso                                0.07  \n",
       "RidgeCV                              0.24  \n",
       "Ridge                                0.03  \n",
       "TransformedTargetRegressor           0.03  \n",
       "LinearRegression                     0.12  \n",
       "KernelRidge                         11.81  \n",
       "Lars                                 0.09  \n",
       "SGDRegressor                         0.04  \n",
       "PoissonRegressor                     0.54  \n",
       "MLPRegressor                        27.21  \n",
       "HistGradientBoostingRegressor      526.78  \n",
       "LGBMRegressor                       55.37  \n",
       "ExtraTreesRegressor                  2.43  \n",
       "RandomForestRegressor               17.00  \n",
       "KNeighborsRegressor                  0.23  \n",
       "XGBRegressor                        63.66  \n",
       "BaggingRegressor                     1.91  \n",
       "GradientBoostingRegressor            3.14  \n",
       "ExtraTreeRegressor                   0.09  \n",
       "DecisionTreeRegressor                0.31  \n",
       "AdaBoostRegressor                    1.17  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/42 [00:54<37:33, 54.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'AdaBoostRegressor', 'R-Squared': -6.27437131841446, 'Adjusted R-Squared': -6.284525382231341, 'RMSE': 9.787981565238232, 'Time taken': 54.95334315299988}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▍         | 2/42 [02:11<45:05, 67.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'BaggingRegressor', 'R-Squared': 0.13014442948374194, 'Adjusted R-Squared': 0.12893022572813584, 'RMSE': 3.384690670821487, 'Time taken': 76.49333715438843}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 3/42 [02:24<27:42, 42.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'BayesianRidge', 'R-Squared': 0.006551402320528088, 'Adjusted R-Squared': 0.005164678984850513, 'RMSE': 3.6171632548953054, 'Time taken': 12.88908314704895}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|▉         | 4/42 [02:34<18:55, 29.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'DecisionTreeRegressor', 'R-Squared': -0.38964014215530596, 'Adjusted R-Squared': -0.3915798966809274, 'RMSE': 4.278057183352493, 'Time taken': 10.30411148071289}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 5/42 [02:35<11:55, 19.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'DummyRegressor', 'R-Squared': -2.2339774563562287e-06, 'Adjusted R-Squared': -0.0013981053258336207, 'RMSE': 3.629074593502688, 'Time taken': 0.6887359619140625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 6/42 [02:36<07:59, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'ElasticNet', 'R-Squared': -2.2339774563562287e-06, 'Adjusted R-Squared': -0.0013981053258336207, 'RMSE': 3.629074593502688, 'Time taken': 1.6298069953918457}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 7/42 [03:18<13:04, 22.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'ElasticNetCV', 'R-Squared': 0.005796424697616653, 'Adjusted R-Squared': 0.0044086475126609015, 'RMSE': 3.618537437053481, 'Time taken': 41.13783526420593}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▉        | 8/42 [03:21<09:11, 16.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'ExtraTreeRegressor', 'R-Squared': -0.6901080674689013, 'Adjusted R-Squared': -0.6924672356256139, 'RMSE': 4.717942539604872, 'Time taken': 2.9797446727752686}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██▏       | 9/42 [08:07<55:23, 100.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'ExtraTreesRegressor', 'R-Squared': 0.10747014220054285, 'Adjusted R-Squared': 0.10622428812767937, 'RMSE': 3.4285207844571817, 'Time taken': 286.4595503807068}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 10/42 [08:08<37:13, 69.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GammaRegressor model failed to execute\n",
      "Some value(s) of y are out of the valid range for family GammaDistribution\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "\n",
    "# build a df to store the information\n",
    "\n",
    "# instead use the subset, this time I am trying the whole, which might be not suitable for ML, but I just wanted a positive control\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "res_df =[]\n",
    "\n",
    "\n",
    "Target = []\n",
    "Dataset = []\n",
    "for i in range(Celltype_mtx_norm.shape[0]):\n",
    "    for j in range(Celltype_mtx_norm.shape[0]):\n",
    "        #print(i)\n",
    "        #print(j)\n",
    "        if i == j:\n",
    "            pass       \n",
    "        else:\n",
    "\n",
    "            #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "            #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "            Dataset.append(np.concatenate((Celltype_mtx_norm[i,:],Celltype_mtx_norm[j,:])))\n",
    "            Target.append(Connectome_direct_density[i,j])\n",
    "Dataset = np.stack(Dataset)\n",
    "Target =np.array([np.array(xi) for xi in Target])   \n",
    "    \n",
    "#X_train, X_test, y_train, y_test = train_test_split(Dataset, Target,test_size=.2,random_state =123)  \n",
    "\n",
    "#\n",
    "kf = KFold(n_splits=5, shuffle= True)\n",
    "\n",
    "for train_index, test_index in kf.split(Dataset):\n",
    "    sub1 = train_index#), int(len(train_index)/50))\n",
    "    sub2 = test_index#), int(len(test_index)/50))\n",
    "    X_train, X_test = Dataset[sub1], Dataset[sub2]\n",
    "    y_train, y_test = Target[sub1], Target[sub2]\n",
    "  \n",
    "    \n",
    "    \n",
    "    reg = LazyRegressor(verbose=1, predictions=True,ignore_warnings=False, custom_metric=None)\n",
    "    models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
    "    print(models)\n",
    "    models.to_csv('res_inproper_all.csv', sep = '\\t')\n",
    "    break\n",
    "# split into 50 %\n",
    "#sub_1 = sample(list(range(X_train.shape[0])),int(X_train.shape[0]/10) )\n",
    "#sub_2 = sample(list(range(X_test.shape[0])),int(X_test.shape[0]/10) )\n",
    "# subset 50 percents\n",
    "#Dataset_Training_sub = X_train[sub_1,:]\n",
    "    \n",
    "#Target_training_sub = y_train[sub_1]\n",
    "    \n",
    "#Dataset_Testing_sub = X_test[sub_2,:]\n",
    "    \n",
    "#Target_Testing_sub = y_test[sub_2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3587,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RandomForestRegressor', 0.004659784996502658, 3.4779960071010225]\n",
      "['RandomForestRegressor', 0.006998202035632062, 3.4902083996326403]\n",
      "['RandomForestRegressor', 0.008233164737332377, 4.030816268865591]\n",
      "['RandomForestRegressor', 0.0065886139845724445, 3.258057572325783]\n",
      "['RandomForestRegressor', 0.007675450180245735, 3.4102158244164]\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "\n",
    "# build a df to store the information\n",
    "\n",
    "# instead use the subset, this time I am trying the whole, which might be not suitable for ML, but I just wanted a positive control\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "res_df =[]\n",
    "\n",
    "\n",
    "Target = []\n",
    "Dataset = []\n",
    "for i in range(Celltype_mtx_norm.shape[0]):\n",
    "    for j in range(Celltype_mtx_norm.shape[0]):\n",
    "        #print(i)\n",
    "        #print(j)\n",
    "        if i == j:\n",
    "            pass       \n",
    "        else:\n",
    "\n",
    "            #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "            #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "            Dataset.append(np.concatenate((Celltype_mtx_norm[i,:],Celltype_mtx_norm[j,:])))\n",
    "            Target.append(Connectome_direct_density[i,j])\n",
    "Dataset = np.stack(Dataset)\n",
    "Target =np.array([np.array(xi) for xi in Target])   \n",
    "    \n",
    "#X_train, X_test, y_train, y_test = train_test_split(Dataset, Target,test_size=.2,random_state =123)  \n",
    "\n",
    "#\n",
    "kf = KFold(n_splits=5, shuffle= True)\n",
    "\n",
    "for train_index, test_index in kf.split(Dataset):\n",
    "    X_train, X_test = Dataset[train_index], Dataset[test_index]\n",
    "    y_train, y_test = Target[train_index], Target[test_index]\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "    Predict = lr.predict(X_test)\n",
    "    _res = []\n",
    "    _res.append('lr')\n",
    "\n",
    "    _res.append(r2_score(y_test,Predict))\n",
    "    _res.append(mean_squared_error(y_test,Predict,squared=False))\n",
    "    print(_res)\n",
    "    \n",
    "    \n",
    "#reg = LazyRegressor(verbose=1, predictions=True,ignore_warnings=False, custom_metric=None)\n",
    "# split into 50 %\n",
    "#sub_1 = sample(list(range(X_train.shape[0])),int(X_train.shape[0]/10) )\n",
    "#sub_2 = sample(list(range(X_test.shape[0])),int(X_test.shape[0]/10) )\n",
    "# subset 50 percents\n",
    "#Dataset_Training_sub = X_train[sub_1,:]\n",
    "    \n",
    "#Target_training_sub = y_train[sub_1]\n",
    "    \n",
    "#Dataset_Testing_sub = X_test[sub_2,:]\n",
    "    \n",
    "#Target_Testing_sub = y_test[sub_2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____\n"
     ]
    }
   ],
   "source": [
    "# SVR\n",
    "\n",
    "# build a df to store the information\n",
    "\n",
    "# instead use the subset, this time I am trying the whole, which might be not suitable for ML, but I just wanted a positive control\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "res_df =[]\n",
    "\n",
    "\n",
    "Target = []\n",
    "Dataset = []\n",
    "for i in range(Celltype_mtx_norm.shape[0]):\n",
    "    for j in range(Celltype_mtx_norm.shape[0]):\n",
    "        #print(i)\n",
    "        #print(j)\n",
    "        if i == j:\n",
    "            pass       \n",
    "        else:\n",
    "\n",
    "            #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "            #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "            Dataset.append(np.concatenate((Celltype_mtx_norm[i,:],Celltype_mtx_norm[j,:])))\n",
    "            Target.append(Connectome_direct_density[i,j])\n",
    "Dataset = np.stack(Dataset)\n",
    "Target =np.array([np.array(xi) for xi in Target])   \n",
    "    \n",
    "#X_train, X_test, y_train, y_test = train_test_split(Dataset, Target,test_size=.2,random_state =123)  \n",
    "\n",
    "#\n",
    "kf = KFold(n_splits=5, shuffle= True)\n",
    "\n",
    "for train_index, test_index in kf.split(Dataset):\n",
    "    print('____')\n",
    "    X_train, X_test = Dataset[train_index], Dataset[test_index]\n",
    "    y_train, y_test = Target[train_index], Target[test_index]\n",
    "    svr = SVR()\n",
    "    svr.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "    Predict = svr.predict(X_test)\n",
    "    _res = []\n",
    "    _res.append('RandomForestRegressor')\n",
    "\n",
    "    _res.append(r2_score(y_test,Predict))\n",
    "    _res.append(mean_squared_error(y_test,Predict,squared=False))\n",
    "    print(_res)\n",
    "    \n",
    "    \n",
    "#reg = LazyRegressor(verbose=1, predictions=True,ignore_warnings=False, custom_metric=None)\n",
    "# split into 50 %\n",
    "#sub_1 = sample(list(range(X_train.shape[0])),int(X_train.shape[0]/10) )\n",
    "#sub_2 = sample(list(range(X_test.shape[0])),int(X_test.shape[0]/10) )\n",
    "# subset 50 percents\n",
    "#Dataset_Training_sub = X_train[sub_1,:]\n",
    "    \n",
    "#Target_training_sub = y_train[sub_1]\n",
    "    \n",
    "#Dataset_Testing_sub = X_test[sub_2,:]\n",
    "    \n",
    "#Target_Testing_sub = y_test[sub_2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96822965, 0.78817663, 0.72479129, 0.82032864, 0.92730691,\n",
       "       0.50795244, 0.82515398, 0.94652056, 1.        , 0.98032987,\n",
       "       0.99550904, 0.97737923, 1.        , 1.        , 0.97699912,\n",
       "       0.98952854, 0.7394642 , 0.59969775, 0.92784213, 0.94694797,\n",
       "       0.91149344, 0.80121812, 0.85247174, 0.76296672, 0.97651273,\n",
       "       0.49770912, 0.92353547, 0.98336786, 0.76338216, 0.93051494,\n",
       "       0.422534  , 0.72074844, 0.91873064, 0.66188623, 0.96662907,\n",
       "       0.92766184, 0.75139376, 0.97489687, 0.93668178, 0.99383866,\n",
       "       0.63769894, 0.80606318, 0.91386461, 0.69750952, 0.99709589,\n",
       "       0.91181692, 0.53702232, 0.72445541, 0.96431617, 0.95015779])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143481, 50)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35871, 50)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RandomForestRegressor', 0.25931770503665563, 3.115926466907411]\n"
     ]
    }
   ],
   "source": [
    "RF_Regmodel = RandomForestRegressor()\n",
    "RF_Regmodel.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "Predict = RF_Regmodel.predict(X_test)\n",
    "_res = []\n",
    "_res.append('RandomForestRegressor')\n",
    "    \n",
    "_res.append(r2_score(y_test,Predict))\n",
    "_res.append(mean_squared_error(y_test,Predict,squared=False))\n",
    "print(_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8865365991304476"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Predict = RF_Regmodel.predict(X_train[:])\n",
    "r2_score(y_train[:],Predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8967,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Target_Testing_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.04290075, 0.05129423, 0.00204868, 0.        ,\n",
       "       0.19620738, 0.        , 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Target_training_sub[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10409774, 3.59019734, 0.24403804, 0.05332101, 0.16795146,\n",
       "       0.3863247 , 0.18396161, 3.47359509, 0.53028246, 1.40108984])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Predict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.60533880e-04, 0.00000000e+00, 1.52469704e-02, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.11590916e-02, 1.58809255e+00])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Target_Testing_sub[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform the validation, iteration:1\n",
      "Build Training set\n",
      "Build Testing set\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "         \n",
    "    reg = LazyRegressor(verbose=1, predictions=True,ignore_warnings=False, custom_metric=None)\n",
    "    # split into 50 %\n",
    "    sub_1 = sample(list(range(Dataset_Training.shape[0])),int(Dataset_Training.shape[0]/40) )\n",
    "    sub_2 = sample(list(range(Dataset_Testing.shape[0])),int(Dataset_Testing.shape[0]/10) )\n",
    "    # subset 50 percents\n",
    "    Dataset_Training_sub = Dataset_Training[sub_1,:]\n",
    "    \n",
    "    Target_training_sub = Target_training[sub_1]\n",
    "    \n",
    "    Dataset_Testing_sub = Dataset_Testing[sub_2,:]\n",
    "    \n",
    "    Target_Testing_sub = Target_Testing[sub_2]\n",
    "    \n",
    "    #models, predictions = reg.fit(Dataset_Training_sub, Dataset_Testing_sub, Target_training_sub, Target_Testing_sub)\n",
    "    #print(models)\n",
    "    \n",
    "    # random forest reg\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    RF_Regmodel = RandomForestRegressor()\n",
    "    RF_Regmodel.fit(Dataset_Training_sub,Target_training_sub)\n",
    "\n",
    "\n",
    "    Predict = RF_Regmodel.predict(Dataset_Testing_sub)\n",
    "    _res = []\n",
    "    _res.append('RandomForestRegressor')\n",
    "    \n",
    "    _res.append(r2_score(Target_Testing_sub,Predict))\n",
    "    _res.append(mean_squared_error(Target_Testing_sub,Predict,squared=False))\n",
    "    res_df.append(_res)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform the validation, iteration:1\n",
      "Build Training set\n",
      "Build Testing set\n"
     ]
    }
   ],
   "source": [
    "# support vector\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "k = 0\n",
    "for train_index, test_index in KF.split(list(range(0,424))):\n",
    "    k = k +1\n",
    "    print('Perform the validation, iteration:'+str(k))\n",
    "    Targets_Train_set = Connectome_direct_density[:,train_index]\n",
    "    Targets_Train_set  = Targets_Train_set[train_index,:]\n",
    "    Targets_Test_set  = Connectome_direct_density[:,test_index]\n",
    "    Targets_Test_set   = Targets_Test_set[test_index,:]\n",
    "    \n",
    "    \n",
    "    Dataset_Train_set = Celltype_mtx_norm[train_index,:]\n",
    "    Dataset_Test_set  = Celltype_mtx_norm[test_index,:]\n",
    "    print('Build Training set')\n",
    "    Target_training = []\n",
    "    Dataset_Training = []\n",
    "    for i in range(Dataset_Train_set.shape[0]):\n",
    "        for j in range(Dataset_Train_set.shape[0]):\n",
    "            #print(i)\n",
    "            #print(j)\n",
    "            if i == j:\n",
    "                pass       \n",
    "            else:\n",
    "\n",
    "                #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "                #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "                Dataset_Training.append(np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:])))\n",
    "                Target_training.append(Targets_Train_set[i,j])\n",
    "    Dataset_Training = np.stack(Dataset_Training)\n",
    "    Target_training =np.array([np.array(xi) for xi in Target_training])   \n",
    "    \n",
    "    print('Build Testing set')\n",
    "    Target_Testing = []\n",
    "    Dataset_Testing = []\n",
    "    for i in range(Dataset_Test_set.shape[0]):\n",
    "        for j in range(Dataset_Test_set.shape[0]):\n",
    "            #print(i)\n",
    "            #print(j)\n",
    "            if i == j:\n",
    "                pass       \n",
    "            else:\n",
    "                #print(Dataset_Test_set[i,j])\n",
    "\n",
    "                #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "                #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "                Dataset_Testing.append(np.concatenate((Dataset_Test_set[i,:],Dataset_Test_set[j,:])))\n",
    "                Target_Testing.append(Targets_Test_set[i,j])\n",
    "    Dataset_Testing = np.stack(Dataset_Testing)\n",
    "    Target_Testing =np.array([np.array(xi) for xi in Target_Testing])      \n",
    "         \n",
    "    reg = LazyRegressor(verbose=1, predictions=True,ignore_warnings=False, custom_metric=None)\n",
    "    # split into 50 %\n",
    "    sub_1 = sample(list(range(Dataset_Training.shape[0])),int(Dataset_Training.shape[0]/40) )\n",
    "    sub_2 = sample(list(range(Dataset_Testing.shape[0])),int(Dataset_Testing.shape[0]/10) )\n",
    "    # subset 50 percents\n",
    "    Dataset_Training_sub = Dataset_Training[sub_1,:]\n",
    "    \n",
    "    Target_training_sub = Target_training[sub_1]\n",
    "    \n",
    "    Dataset_Testing_sub = Dataset_Testing[sub_2,:]\n",
    "    \n",
    "    Target_Testing_sub = Target_Testing[sub_2]\n",
    "    \n",
    "    #models, predictions = reg.fit(Dataset_Training_sub, Dataset_Testing_sub, Target_training_sub, Target_Testing_sub)\n",
    "    #print(models)\n",
    "    \n",
    "    # support vector regressor\n",
    "    \n",
    "    \n",
    "    svr_rbf = SVR()\n",
    "    \n",
    "    svr_rbf.fit(Dataset_Training_sub,Target_training_sub)\n",
    "\n",
    "\n",
    "    Predict = svr_rbf.predict(Dataset_Testing_sub)\n",
    "    _res = []\n",
    "    _res.append('SVR')\n",
    "    \n",
    "    _res.append(r2_score(Target_Testing_sub,Predict))\n",
    "    _res.append(mean_squared_error(Target_Testing_sub,Predict,squared=False))\n",
    "    res_df.append(_res)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform the validation, iteration:1\n",
      "Build Training set\n",
      "Build Testing set\n"
     ]
    }
   ],
   "source": [
    "# linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "k = 0\n",
    "for train_index, test_index in KF.split(list(range(0,424))):\n",
    "    k = k +1\n",
    "    print('Perform the validation, iteration:'+str(k))\n",
    "    Targets_Train_set = Connectome_direct_density[:,train_index]\n",
    "    Targets_Train_set  = Targets_Train_set[train_index,:]\n",
    "    Targets_Test_set  = Connectome_direct_density[:,test_index]\n",
    "    Targets_Test_set   = Targets_Test_set[test_index,:]\n",
    "    \n",
    "    \n",
    "    Dataset_Train_set = Celltype_mtx_norm[train_index,:]\n",
    "    Dataset_Test_set  = Celltype_mtx_norm[test_index,:]\n",
    "    print('Build Training set')\n",
    "    Target_training = []\n",
    "    Dataset_Training = []\n",
    "    for i in range(Dataset_Train_set.shape[0]):\n",
    "        for j in range(Dataset_Train_set.shape[0]):\n",
    "            #print(i)\n",
    "            #print(j)\n",
    "            if i == j:\n",
    "                pass       \n",
    "            else:\n",
    "\n",
    "                #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "                #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "                Dataset_Training.append(np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:])))\n",
    "                Target_training.append(Targets_Train_set[i,j])\n",
    "    Dataset_Training = np.stack(Dataset_Training)\n",
    "    Target_training =np.array([np.array(xi) for xi in Target_training])   \n",
    "    \n",
    "    print('Build Testing set')\n",
    "    Target_Testing = []\n",
    "    Dataset_Testing = []\n",
    "    for i in range(Dataset_Test_set.shape[0]):\n",
    "        for j in range(Dataset_Test_set.shape[0]):\n",
    "            #print(i)\n",
    "            #print(j)\n",
    "            if i == j:\n",
    "                pass       \n",
    "            else:\n",
    "                #print(Dataset_Test_set[i,j])\n",
    "\n",
    "                #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "                #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "                Dataset_Testing.append(np.concatenate((Dataset_Test_set[i,:],Dataset_Test_set[j,:])))\n",
    "                Target_Testing.append(Targets_Test_set[i,j])\n",
    "    Dataset_Testing = np.stack(Dataset_Testing)\n",
    "    Target_Testing =np.array([np.array(xi) for xi in Target_Testing])      \n",
    "         \n",
    "    reg = LazyRegressor(verbose=1, predictions=True,ignore_warnings=False, custom_metric=None)\n",
    "    # split into 50 %\n",
    "    sub_1 = sample(list(range(Dataset_Training.shape[0])),int(Dataset_Training.shape[0]/40) )\n",
    "    sub_2 = sample(list(range(Dataset_Testing.shape[0])),int(Dataset_Testing.shape[0]/10) )\n",
    "    # subset 50 percents\n",
    "    Dataset_Training_sub = Dataset_Training[sub_1,:]\n",
    "    \n",
    "    Target_training_sub = Target_training[sub_1]\n",
    "    \n",
    "    Dataset_Testing_sub = Dataset_Testing[sub_2,:]\n",
    "    \n",
    "    Target_Testing_sub = Target_Testing[sub_2]\n",
    "    \n",
    "    #models, predictions = reg.fit(Dataset_Training_sub, Dataset_Testing_sub, Target_training_sub, Target_Testing_sub)\n",
    "    #print(models)\n",
    "    \n",
    "    # support vector regressor\n",
    "    \n",
    "    \n",
    "    Lr = LinearRegression()\n",
    "    \n",
    "    Lr.fit(Dataset_Training_sub,Target_training_sub)\n",
    "\n",
    "\n",
    "    Predict = Lr.predict(Dataset_Testing_sub)\n",
    "    _res = []\n",
    "    _res.append('LinearRegression')\n",
    "    \n",
    "    _res.append(r2_score(Target_Testing_sub,Predict))\n",
    "    _res.append(mean_squared_error(Target_Testing_sub,Predict,squared=False))\n",
    "    res_df.append(_res)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "k = 0\n",
    "for train_index, test_index in KF.split(list(range(0,424))):\n",
    "    k = k +1\n",
    "    print('Perform the validation, iteration:'+str(k))\n",
    "    Targets_Train_set = Connectome_direct_density[:,train_index]\n",
    "    Targets_Train_set  = Targets_Train_set[train_index,:]\n",
    "    Targets_Test_set  = Connectome_direct_density[:,test_index]\n",
    "    Targets_Test_set   = Targets_Test_set[test_index,:]\n",
    "    \n",
    "    \n",
    "    Dataset_Train_set = Celltype_mtx_norm[train_index,:]\n",
    "    Dataset_Test_set  = Celltype_mtx_norm[test_index,:]\n",
    "    print('Build Training set')\n",
    "    Target_training = []\n",
    "    Dataset_Training = []\n",
    "    for i in range(Dataset_Train_set.shape[0]):\n",
    "        for j in range(Dataset_Train_set.shape[0]):\n",
    "            #print(i)\n",
    "            #print(j)\n",
    "            if i == j:\n",
    "                pass       \n",
    "            else:\n",
    "\n",
    "                #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "                #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "                Dataset_Training.append(np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:])))\n",
    "                Target_training.append(Targets_Train_set[i,j])\n",
    "    Dataset_Training = np.stack(Dataset_Training)\n",
    "    Target_training =np.array([np.array(xi) for xi in Target_training])   \n",
    "    \n",
    "    print('Build Testing set')\n",
    "    Target_Testing = []\n",
    "    Dataset_Testing = []\n",
    "    for i in range(Dataset_Test_set.shape[0]):\n",
    "        for j in range(Dataset_Test_set.shape[0]):\n",
    "            #print(i)\n",
    "            #print(j)\n",
    "            if i == j:\n",
    "                pass       \n",
    "            else:\n",
    "                #print(Dataset_Test_set[i,j])\n",
    "\n",
    "                #_Dataset_Training = np.concatenate((Dataset_Train_set[i,:],Dataset_Train_set[j,:]))\n",
    "                #Dataset_Training = np.stack((Dataset_Training,_Dataset_Training))\n",
    "                Dataset_Testing.append(np.concatenate((Dataset_Test_set[i,:],Dataset_Test_set[j,:])))\n",
    "                Target_Testing.append(Targets_Test_set[i,j])\n",
    "    Dataset_Testing = np.stack(Dataset_Testing)\n",
    "    Target_Testing =np.array([np.array(xi) for xi in Target_Testing])      \n",
    "         \n",
    "    reg = LazyRegressor(verbose=1, predictions=True,ignore_warnings=False, custom_metric=None)\n",
    "    # split into 50 %\n",
    "    sub_1 = sample(list(range(Dataset_Training.shape[0])),int(Dataset_Training.shape[0]/40) )\n",
    "    sub_2 = sample(list(range(Dataset_Testing.shape[0])),int(Dataset_Testing.shape[0]/10) )\n",
    "    # subset 50 percents\n",
    "    Dataset_Training_sub = Dataset_Training[sub_1,:]\n",
    "    \n",
    "    Target_training_sub = Target_training[sub_1]\n",
    "    \n",
    "    Dataset_Testing_sub = Dataset_Testing[sub_2,:]\n",
    "    \n",
    "    Target_Testing_sub = Target_Testing[sub_2]\n",
    "    \n",
    "    #models, predictions = reg.fit(Dataset_Training_sub, Dataset_Testing_sub, Target_training_sub, Target_Testing_sub)\n",
    "    #print(models)\n",
    "    \n",
    "    # support vector regressor\n",
    "    \n",
    "    \n",
    "    Lr = LinearRegression()\n",
    "    \n",
    "    Lr.fit(Dataset_Training_sub,Target_training_sub)\n",
    "\n",
    "\n",
    "    Predict = Lr.predict(Dataset_Testing_sub)\n",
    "    _res = []\n",
    "    _res.append('LinearRegression')\n",
    "    \n",
    "    _res.append(r2_score(Target_Testing_sub,Predict))\n",
    "    _res.append(mean_squared_error(Target_Testing_sub,Predict,squared=False))\n",
    "    res_df.append(_res)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Methods</th>\n",
       "      <th>R_square</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>-2.17</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVR</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>1.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Methods  R_square  RMSE\n",
       "0  RandomForestRegressor     -2.17  2.75\n",
       "1                    SVR     -0.01  2.75\n",
       "2       LinearRegression     -0.09  1.73"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Result = pd.DataFrame(res_df,columns = ['Methods','R_square','RMSE'])\n",
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_Testing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\n",
    "models, predictions = reg.fit(Dataset_Training, Dataset_Testing, Target_training, Target_Testing)\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
